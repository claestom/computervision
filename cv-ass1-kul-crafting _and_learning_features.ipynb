{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n%pip install opencv-python\n%pip install cmake\n%pip install dlib\n%pip install face_recognition\n%pip install --user insightface\n%pip install --user onnxruntime-gpu\n%pip install keras_facenet\n%pip install tensorflow\n%pip install imgaug\n%pip install xgboost\n\n\nimport io # Input/Output Module\nimport os # OS interfaces\nimport pathlib\nimport cv2 # OpenCV package\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.notebook import tqdm\n\nimport face_recognition as fr\nimport insightface\nfrom insightface.app import FaceAnalysis\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_validate, ShuffleSplit, GridSearchCV, LeaveOneOut \nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom skimage.transform import resize\nfrom skimage.feature import hog\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial import distance\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport imgaug as ia\n\nfrom xgboost import XGBClassifier\n\nfrom math import ceil\nfrom urllib import request # module for opening HTTP requests\nfrom matplotlib import pyplot as plt # Plotting library\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox # PCA Feature space plot\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"bR1g7zkorENA","papermill":{"duration":0.230891,"end_time":"2021-03-08T07:57:06.335029","exception":false,"start_time":"2021-03-08T07:57:06.104138","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:46:06.088276Z","iopub.execute_input":"2022-04-12T20:46:06.088815Z","iopub.status.idle":"2022-04-12T20:48:56.827245Z","shell.execute_reply.started":"2022-04-12T20:46:06.088704Z","shell.execute_reply":"2022-04-12T20:48:56.826433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"width:100%; height:140px\">\n    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n</div>\n\n\nKUL H02A5a Computer Vision: Group Assignment 1\n---------------------------------------------------------------\nStudent numbers: <span style=\"color:red\">r0843426, r0670841, r0673254, r0870074, r5</span>.\n\nThe goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n\nIn this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n\n---------------------------------------------------------------\nThis notebook is structured as follows:\n0. Data loading & Preprocessing\n1. Feature Representations\n2. Evaluation Metrics \n3. Classifiers\n4. Experiments\n5. Publishing best results\n6. Discussion\n\nMake sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n\nFill in your student numbers above and get to it! Good luck! \n\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n</div>\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n</div>\n    \n","metadata":{"id":"HgXdV6_VrENF","papermill":{"duration":0.022868,"end_time":"2021-03-08T07:57:06.382109","exception":false,"start_time":"2021-03-08T07:57:06.359241","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---------------------------------------------------------------\n# 0. Data loading & Preprocessing\n\n## 0.1. Loading data\nThe training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! ","metadata":{"id":"FCI9BTe0rENH"}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n\ntrain = pd.read_csv(\n    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train_set.csv', index_col = 0)\ntrain.index = train.index.rename('id')\n\ntest = pd.read_csv(\n    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test_set.csv', index_col = 0)\ntest.index = test.index.rename('id')\n\n# read the images as numpy arrays and store in \"img\" column\ntrain['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in train.iterrows()]\n\ntest['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in test.iterrows()]\n  \n\ntrain_size, test_size = len(train),len(test)\n\n\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)","metadata":{"id":"JZNfKQzzrENI","papermill":{"duration":37.543619,"end_time":"2021-03-08T07:57:43.9495","exception":false,"start_time":"2021-03-08T07:57:06.405881","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:48:56.829007Z","iopub.execute_input":"2022-04-12T20:48:56.829589Z","iopub.status.idle":"2022-04-12T20:49:17.469019Z","shell.execute_reply.started":"2022-04-12T20:48:56.829552Z","shell.execute_reply":"2022-04-12T20:49:17.468244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 0.2. Downloading prep_data","metadata":{}},{"cell_type":"code","source":"!conda install -y gdown\n!gdown 1TdJCUfysjTe59l49QMZmqAk_xXDUOK5f\n\nimport tarfile\n\nfile = tarfile.open('/kaggle/working/prep.tar.gz', \"r:gz\")\nfile.extractall()\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:49:17.470436Z","iopub.execute_input":"2022-04-12T20:49:17.470659Z","iopub.status.idle":"2022-04-12T20:49:54.653806Z","shell.execute_reply.started":"2022-04-12T20:49:17.470632Z","shell.execute_reply":"2022-04-12T20:49:54.652895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n\n## 0.3. A first look\nLet's have a look at the data columns and class distribution.","metadata":{"id":"i--QdNFBrENI","papermill":{"duration":0.023377,"end_time":"2021-03-08T07:57:43.997466","exception":false,"start_time":"2021-03-08T07:57:43.974089","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# The training set contains an identifier, name, image information and class label\ntrain.head(1)","metadata":{"id":"-fjWHAxNrENJ","papermill":{"duration":3.315629,"end_time":"2021-03-08T07:57:47.336913","exception":false,"start_time":"2021-03-08T07:57:44.021284","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:49:54.657582Z","iopub.execute_input":"2022-04-12T20:49:54.657899Z","iopub.status.idle":"2022-04-12T20:49:57.054096Z","shell.execute_reply.started":"2022-04-12T20:49:54.657859Z","shell.execute_reply":"2022-04-12T20:49:57.053477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The test set only contains an identifier and corresponding image information.\ntest.head(1)","metadata":{"id":"WihI7OY-rENJ","papermill":{"duration":3.283501,"end_time":"2021-03-08T07:57:50.644778","exception":false,"start_time":"2021-03-08T07:57:47.361277","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:49:57.055239Z","iopub.execute_input":"2022-04-12T20:49:57.055558Z","iopub.status.idle":"2022-04-12T20:50:00.234098Z","shell.execute_reply.started":"2022-04-12T20:49:57.05552Z","shell.execute_reply":"2022-04-12T20:50:00.233035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The class distribution in the training set:\ntrain.groupby('name').agg({'img':'count', 'class': 'max'})","metadata":{"id":"cM0I7X2srENK","papermill":{"duration":0.046628,"end_time":"2021-03-08T07:57:50.716317","exception":false,"start_time":"2021-03-08T07:57:50.669689","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:00.235535Z","iopub.execute_input":"2022-04-12T20:50:00.235772Z","iopub.status.idle":"2022-04-12T20:50:00.254887Z","shell.execute_reply.started":"2022-04-12T20:50:00.235742Z","shell.execute_reply":"2022-04-12T20:50:00.253949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. ","metadata":{"id":"XzZFxr7trENK"}},{"cell_type":"markdown","source":"### 0.3.1 Visualizing the original dataset","metadata":{"id":"mglhaAXRrENL"}},{"cell_type":"code","source":"FACE_SIZE = (100,100)\n\ndef plot_image_sequence(data, n, imgs_per_row=10):\n    n_rows = ceil(n/(imgs_per_row))\n    n_cols = min(imgs_per_row, n)\n\n    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n    for i in range(n):\n        if n == 1:\n            ax.imshow(data[i])\n        elif n_rows > 1:\n            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i])\n        else:\n            ax[int(i%n)].imshow(data[i])\n    plt.show()\n    \ndef plot_gray_image_sequence(data, n, imgs_per_row=10):\n    n_rows = ceil(n/(imgs_per_row))\n    n_cols = min(imgs_per_row, n)\n\n    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n    for i in range(n):\n        if n == 1:\n            ax.imshow(data[i], cmap = 'gray')\n        elif n_rows > 1:\n            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i],cmap = 'gray')\n        else:\n            ax[int(i%n)].imshow(data[i], cmap = 'gray')\n    plt.show()","metadata":{"id":"2Bavn1FFrENL","execution":{"iopub.status.busy":"2022-04-12T20:50:00.256577Z","iopub.execute_input":"2022-04-12T20:50:00.256832Z","iopub.status.idle":"2022-04-12T20:50:00.266425Z","shell.execute_reply.started":"2022-04-12T20:50:00.256793Z","shell.execute_reply":"2022-04-12T20:50:00.26576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = train['img'].to_numpy()\nplot_image_sequence(train_images[60:80], n=20)","metadata":{"id":"hgsNws4IrENM","execution":{"iopub.status.busy":"2022-04-12T20:50:00.267609Z","iopub.execute_input":"2022-04-12T20:50:00.268356Z","iopub.status.idle":"2022-04-12T20:50:05.902372Z","shell.execute_reply.started":"2022-04-12T20:50:00.268307Z","shell.execute_reply":"2022-04-12T20:50:05.901339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By looking at the original training dataset, we can spot some issues which might occur when trying to classify faces.<br>\n\n**1)** Some images contain multiple faces. We will have to make a decision on how to approach this problem. One possible solution would be to detect only the first face that is found in the image. Another possible solution would be to split the image for every face. This would mean that the training dataset would increase in size. However, it would also result in some false positives as each face would be given the same label that the original image had.<br>  \n**2)** There is one image which is unable to load. When classifying the image it might be a good idea to leave this particular case out of the training dataset.","metadata":{"id":"mwnymhoUrENM"}},{"cell_type":"markdown","source":"## 0.4. Preprocess data\n### 0.4.1 HAAR face detector\nIn this example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces, then the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one. \n\n<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n</div>\n","metadata":{"id":"ooBu4yUjrENN","papermill":{"duration":0.025108,"end_time":"2021-03-08T07:57:50.766719","exception":false,"start_time":"2021-03-08T07:57:50.741611","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class HAARPreprocessor():\n    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n    \n    def __init__(self, path, face_size, faces=-1):\n        self.face_size = face_size\n        self.faces = faces\n        if faces == 1:\n            self.name = 'HAAR_FIRST'\n        else:\n            self.name = 'HAAR_MULTI'\n            \n        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n        if not os.path.exists(file_path): \n            if not os.path.exists(path):\n                os.mkdir(path)\n            self.download_model(file_path)\n        \n        self.classifier = cv2.CascadeClassifier(file_path)\n  \n    def download_model(self, path):\n        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n            \"haarcascades/haarcascade_frontalface_default.xml\"\n        \n        with request.urlopen(url) as r, open(path, 'wb') as f:\n            f.write(r.read())\n            \n    def detect_faces(self, img):\n        \"\"\"Detect all faces in an image.\"\"\"\n        \n        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        return self.classifier.detectMultiScale(\n            img_gray,\n            scaleFactor=1.2,\n            minNeighbors=5,\n            minSize=(30, 30),\n            flags=cv2.CASCADE_SCALE_IMAGE\n        )\n        \n    def extract_faces(self, img):\n        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n        \n        faces = self.detect_faces(img)\n\n        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n    \n    def preprocess(self, data_row):\n        faces = self.extract_faces(data_row['img'])\n        \n        # if no faces were found, return None\n        if len(faces) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = np.nan\n            if self.faces == 1:\n                return nan_img\n            else:\n                return np.array([nan_img])\n        \n        if self.faces == 1:\n            return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n        else:\n            return np.array([cv2.resize(face, self.face_size, interpolation = cv2.INTER_AREA) for face in faces])\n    \n    def check_prep(self, label, prep_path):\n        pathlib.Path(prep_path).mkdir(parents=True, exist_ok=True)\n        \n        bool_X = pathlib.Path(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name))).exists()\n        bool_y = pathlib.Path(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name))).exists()\n        bool_ids = pathlib.Path(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name))).exists()\n        return (bool_X and bool_y and bool_ids)\n    \n    def __call__(self, data, values=None, label='train'):\n        prep_path = \"/kaggle/working/prepped_data/1. Detection/\" + self.name\n        if self.check_prep(label, prep_path):\n            print('Loading prepped data...')\n            X = np.load(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name)))\n            y = np.load(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name)))\n            ids = np.load(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name)))\n        else:\n            face_arr = []\n            class_values = []\n            ids = []\n            for i, row in data.iterrows():\n                if self.faces == 1:\n                    face_arr.append(self.preprocess(row))\n                else:\n                    for face in self.preprocess(row):\n                        if values is not None:\n                            ids.append(i)\n                            class_values.append(values[i])\n                        face_arr.append(face)\n            if self.faces == 1 and values is not None:\n                ids = list(range(len(values)))\n                class_values = values\n                \n            X = np.stack(face_arr).astype(int)\n            y = np.array(class_values)\n            ids = np.array(ids)\n            \n            np.save(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name)), X)\n            np.save(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name)), y)\n            np.save(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name)), ids)\n        return X, y, ids","metadata":{"id":"JXnmVGgNrENN","papermill":{"duration":0.042776,"end_time":"2021-03-08T07:57:50.834913","exception":false,"start_time":"2021-03-08T07:57:50.792137","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:05.904082Z","iopub.execute_input":"2022-04-12T20:50:05.90482Z","iopub.status.idle":"2022-04-12T20:50:05.926109Z","shell.execute_reply.started":"2022-04-12T20:50:05.904781Z","shell.execute_reply":"2022-04-12T20:50:05.925284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 0.4.1.1 HAAR face detector (First face detection)","metadata":{"id":"rVKzujXprENO"}},{"cell_type":"code","source":"%%time\npreprocessor = HAARPreprocessor(path = '../../tmp', face_size=FACE_SIZE, faces=1)\n\ntrain_X_HAAR, train_y_HAAR, HAAR_ids = preprocessor(train, train['class'].values, label='train')\n#test_X_HAAR = preprocessor(test)\nprint(f'Preprocessing resulted in {train_X_HAAR.shape[0]} images with {train_y_HAAR.shape[0]} labels.')","metadata":{"id":"FYLfEafYrENO","papermill":{"duration":62.263517,"end_time":"2021-03-08T07:58:53.174859","exception":false,"start_time":"2021-03-08T07:57:50.911342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:05.927914Z","iopub.execute_input":"2022-04-12T20:50:05.92849Z","iopub.status.idle":"2022-04-12T20:50:06.643709Z","shell.execute_reply.started":"2022-04-12T20:50:05.928452Z","shell.execute_reply":"2022-04-12T20:50:06.6428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualise**\n\nLet's plot a few examples.","metadata":{"id":"CRju5se6rENO","papermill":{"duration":0.025332,"end_time":"2021-03-08T07:57:50.885849","exception":false,"start_time":"2021-03-08T07:57:50.860517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# plot faces of Michael and Sarah\nplot_image_sequence(train_X_HAAR[train_y_HAAR == 0], n=20, imgs_per_row=10)","metadata":{"id":"i0eM990qrENP","papermill":{"duration":2.635787,"end_time":"2021-03-08T07:58:55.836611","exception":false,"start_time":"2021-03-08T07:58:53.200824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:06.645137Z","iopub.execute_input":"2022-04-12T20:50:06.645404Z","iopub.status.idle":"2022-04-12T20:50:09.180627Z","shell.execute_reply.started":"2022-04-12T20:50:06.645371Z","shell.execute_reply":"2022-04-12T20:50:09.179646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Jesse\nplot_image_sequence(train_X_HAAR[train_y_HAAR == 1], n=30, imgs_per_row=10)","metadata":{"id":"ghB2NAKsrENP","papermill":{"duration":3.840961,"end_time":"2021-03-08T07:58:59.72249","exception":false,"start_time":"2021-03-08T07:58:55.881529","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:09.181973Z","iopub.execute_input":"2022-04-12T20:50:09.183347Z","iopub.status.idle":"2022-04-12T20:50:12.784261Z","shell.execute_reply.started":"2022-04-12T20:50:09.183297Z","shell.execute_reply":"2022-04-12T20:50:12.783308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Mila\nplot_image_sequence(train_X_HAAR[train_y_HAAR == 2], n=30, imgs_per_row=10)","metadata":{"id":"bsl8iJ4erENP","papermill":{"duration":3.910256,"end_time":"2021-03-08T07:59:03.703299","exception":false,"start_time":"2021-03-08T07:58:59.793043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:12.788639Z","iopub.execute_input":"2022-04-12T20:50:12.788986Z","iopub.status.idle":"2022-04-12T20:50:16.531672Z","shell.execute_reply.started":"2022-04-12T20:50:12.78894Z","shell.execute_reply":"2022-04-12T20:50:16.53062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These images clearly indicate that the preprocessing is not perfect. It seems that in three images it was unable to recognize a face and as such it has returned a blank image.<br>\nFurthermore, there are five images where it has \"detected\" a face, however it seems to have gotten a false positive as they are focused on either the collar of a shirt or on the hair.<br>\nFinally there also seems to be a problem by recognizing the wrong face. This means that, in this case, it will train on data that is not completely reliable. This problem can be partially mitigated by allowing the preprocessing to recognize more than one face.","metadata":{"id":"mYy0F8nZrENP"}},{"cell_type":"markdown","source":"#### 0.4.1.2 HAAR face detector (Multi face detection)","metadata":{"id":"SX8rh4hRrENQ"}},{"cell_type":"code","source":"%%time\npreprocessor = HAARPreprocessor(path = '../../tmp', face_size=FACE_SIZE)\n\ntrain_X_HAAR, train_y_HAAR, HAAR_ids = preprocessor(train, train['class'].values, label='train')\n#test_X_HAAR = preprocessor(test)\nprint(f'Preprocessing resulted in {train_X_HAAR.shape[0]} images with {train_y_HAAR.shape[0]} labels.')","metadata":{"id":"V64jUS4qrENQ","papermill":{"duration":62.263517,"end_time":"2021-03-08T07:58:53.174859","exception":false,"start_time":"2021-03-08T07:57:50.911342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:16.533057Z","iopub.execute_input":"2022-04-12T20:50:16.533452Z","iopub.status.idle":"2022-04-12T20:50:16.574913Z","shell.execute_reply.started":"2022-04-12T20:50:16.53341Z","shell.execute_reply":"2022-04-12T20:50:16.574016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualise**\n\nLet's plot a few examples.","metadata":{"id":"kLP9iNqbrENQ","papermill":{"duration":0.025332,"end_time":"2021-03-08T07:57:50.885849","exception":false,"start_time":"2021-03-08T07:57:50.860517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plot_image_sequence(train_X_HAAR, n=train_X_HAAR.shape[0], imgs_per_row=13)","metadata":{"id":"fTANlZGBrENQ","papermill":{"duration":2.635787,"end_time":"2021-03-08T07:58:55.836611","exception":false,"start_time":"2021-03-08T07:58:53.200824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:16.576111Z","iopub.execute_input":"2022-04-12T20:50:16.576367Z","iopub.status.idle":"2022-04-12T20:50:28.451341Z","shell.execute_reply.started":"2022-04-12T20:50:16.576337Z","shell.execute_reply":"2022-04-12T20:50:28.450221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The total train_X dataset has been expanded from 80 images to 104 images by allowing the preprocessor to recognize more than one face. This means that the dataset should be complete and for every one of the 80 labels, there should exist one picture denoting the right face (With the exception of the pictures where no face or the wrong area was detected). This can be followed up on during the feature extraction by plotting the similarities between images and dismissing all images that are too dissimilar.","metadata":{"id":"rzMZOrBprENQ"}},{"cell_type":"markdown","source":"### 0.4.2: FaceRecognition\nThis preprocessor will make use of the face_recognition package (which can be found [here](https://pypi.org/project/face-recognition/)). <br>\nFor the use of this project this package will only be used in a face detection setting. It is able to detect faces within an image based on two different models: HoG or deep learning (CNN). Whilst the second model should provide better results, it also uses a lot more computation power and is usually paired with GPU acceleration. <br>\nAs such, for this project, the face detection will be applied within a HoG model. ","metadata":{"id":"Sri18KjwrENQ"}},{"cell_type":"code","source":"class FaceRec_Preprocessor():\n    \"\"\"Preprocessing pipeline built around the face-recognition package built on dlib. \"\"\"\n    def __init__(self, face_size, model='hog', faces=1):\n        self.name = 'FACEREC'\n        self.face_size = face_size                                     \n        self.model = model\n        self.faces = faces\n\n    def detect_faces(self, img):\n        \"\"\"Detects all faces in an image\"\"\"        \n        return fr.face_locations(img, model=self.model)\n        \n    def extract_faces(self, img):\n        \"\"\"Returns cropped faces\"\"\"\n        faces = self.detect_faces(img)\n\n        # return the cropped images\n        return [img[top:bottom, left:right] for (top, right, bottom, left) in faces]\n    \n    def preprocess(self, data_row):\n        \"\"\"Returns the cropped images resized to prespecified face_size\"\"\"\n        faces = self.extract_faces(data_row['img']) \n        # if no faces were found, return None\n        if len(faces) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = 0\n            if self.faces == 1:\n                return nan_img\n            else:\n                return np.array([nan_img])\n        \n        if self.faces == 1:\n            return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n        else:\n            return np.array([cv2.resize(face, self.face_size, interpolation = cv2.INTER_AREA) for face in faces])\n    \n    def check_prep(self, label, prep_path):\n        \"\"\"Check whether pre-existing data exists.\"\"\"\n        pathlib.Path(prep_path).mkdir(parents=True, exist_ok=True)\n        \n        bool_X = pathlib.Path(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name))).exists()\n        bool_y = pathlib.Path(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name))).exists()\n        bool_ids = pathlib.Path(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name))).exists()\n        return (bool_X and bool_y and bool_ids)\n    \n    def __call__(self, data, values=None, label='train'):\n        prep_path = \"/kaggle/working/prepped_data/1. Detection/\" + self.name\n        if self.check_prep(label, prep_path):\n            print('Loading prepped data...')\n            X = np.load(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name)))\n            y = np.load(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name)))\n            ids = np.load(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name)))\n        else:\n            face_arr = []\n            class_values = []\n            ids = []\n            for i, row in tqdm(data.iterrows()):\n                if self.faces == 1:\n                    face_arr.append(self.preprocess(row))\n                else:\n                    for face in self.preprocess(row):\n                        ids.append(i)\n                        if values is not None:                        \n                            class_values.append(values[i])\n                        face_arr.append(face)\n            if self.faces == 1 and values is not None:\n                ids = list(range(len(values)))\n                class_values = values\n                \n            X = np.stack(face_arr).astype(int)\n            y = np.array(class_values)\n            ids = np.array(ids)\n            \n            np.save(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name)), X)\n            np.save(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name)), y)\n            np.save(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name)), ids)\n        return X, y, ids","metadata":{"id":"X5YtPH7trENR","execution":{"iopub.status.busy":"2022-04-12T20:50:28.45314Z","iopub.execute_input":"2022-04-12T20:50:28.453708Z","iopub.status.idle":"2022-04-12T20:50:28.824964Z","shell.execute_reply.started":"2022-04-12T20:50:28.453665Z","shell.execute_reply":"2022-04-12T20:50:28.823754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npreprocessor = FaceRec_Preprocessor(FACE_SIZE, faces=-1)\n\ntrain_X_FACEREC, train_y_FACEREC, FACEREC_ids = preprocessor(train, train['class'].values, label='train')\nprint(f'Preprocessing resulted in {train_X_FACEREC.shape[0]} images with {train_y_FACEREC.shape[0]} labels.')","metadata":{"id":"T_4-h6s_rENR","execution":{"iopub.status.busy":"2022-04-12T20:50:28.826311Z","iopub.execute_input":"2022-04-12T20:50:28.826833Z","iopub.status.idle":"2022-04-12T20:50:28.850933Z","shell.execute_reply.started":"2022-04-12T20:50:28.826784Z","shell.execute_reply":"2022-04-12T20:50:28.849991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_image_sequence(train_X_FACEREC, n = 20, imgs_per_row=10)","metadata":{"id":"ppp2Y5bprENR","execution":{"iopub.status.busy":"2022-04-12T20:50:28.852582Z","iopub.execute_input":"2022-04-12T20:50:28.852885Z","iopub.status.idle":"2022-04-12T20:50:31.106819Z","shell.execute_reply.started":"2022-04-12T20:50:28.852844Z","shell.execute_reply":"2022-04-12T20:50:31.105902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that by using this face detection package, we are already able to get much better results. By detecting multiple faces, the dataset has been expanded to 97 images. <br>\nIn 96 of these images we can verify that the dataset was indeed able to recognize a face. In one image it has even managed to identify a face which is really blurry.","metadata":{"id":"Erem70_ulYjM"}},{"cell_type":"markdown","source":"### 0.4.3: InsightFace\nAnother preprocessing technique that can be used is the InsightFace package (found [here](https://pypi.org/project/insightface/)). <br>According to the developers “InsightFace is an integrated Python library for 2D&3D face analysis.” \\[[1](https://insightface.ai/)]. <br>It is able to perform multiple tasks: face detection, face alignment and face recognition. Again, for this project, we will focus on the face detection.","metadata":{"id":"guk85ttcrENS"}},{"cell_type":"code","source":"class InsightFace_Preprocessor():\n    def __init__(self, face_size, faces=-1):\n        self.name = 'INSIGHT'\n        self.face_size = face_size\n        self.faces = faces\n        \n    def detect_faces(self, img):\n        features = self.app.get(img)\n        if features == []:\n            return []\n        face_loc = [list(map(lambda x: 0 if x < 0 else x, f['bbox'].astype(int))) for f in features]\n        return face_loc\n    \n    def extract_faces(self, img):\n        \"\"\"Returns faces (cropped) in an image\"\"\"\n        faces = self.detect_faces(img)                                  # get locations of detected faces\n\n        # crop the images accordingly and save them to a list\n        face = [img[top:bottom, left:right] for (left, top, right, bottom) in faces]\n        return face                                                        # return the list of the cropped images    \n    \n    def preprocess(self, data_row):\n        faces = self.extract_faces(data_row['img'])                     # get list of cropped images     \n        # if no faces were found, return None\n        if len(faces) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = 0\n            if self.faces == 1:\n                return nan_img\n            else:\n                return np.array([nan_img])\n        \n        if self.faces == 1:\n            return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n        else:\n            return np.array([cv2.resize(face, self.face_size, interpolation = cv2.INTER_AREA) for face in faces])\n    \n    def check_prep(self, label, prep_path):\n        pathlib.Path(prep_path).mkdir(parents=True, exist_ok=True)\n        \n        bool_X = pathlib.Path(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name))).exists()\n        bool_y = pathlib.Path(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name))).exists()\n        bool_ids = pathlib.Path(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name))).exists()\n        return (bool_X and bool_y and bool_ids)\n\n    def __call__(self, data, values=None, label='train'):\n        prep_path = \"/kaggle/working/prepped_data/1. Detection/INSIGHT\"\n        if self.check_prep(label, prep_path):\n            print('Loading prepped data...')\n            X = np.load(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name)))\n            y = np.load(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name)))\n            ids = np.load(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name)))\n        else:\n            self.app = FaceAnalysis(allowed_modules=['detection'],providers=['CPUExecutionProvider'])\n            self.app.prepare(ctx_id=0)\n            face_arr = []\n            class_values = []\n            ids = []\n            for i, row in data.iterrows():\n                if self.faces == 1:\n                    face_arr.append(self.preprocess(row))\n                else:\n                    for face in self.preprocess(row):\n                        ids.append(i)\n                        if values is not None:                        \n                            class_values.append(values[i])\n                        face_arr.append(face)\n            if self.faces == 1 and values is not None:\n                ids = list(range(len(values)))\n                class_values = values\n            X = np.stack(face_arr).astype(int)\n            y = np.array(class_values)\n            ids = np.array(ids)\n            \n            np.save(os.path.join(prep_path, '{}_X_{}.npy'.format(label, self.name)), X)\n            np.save(os.path.join(prep_path, '{}_y_{}.npy'.format(label, self.name)), y)\n            np.save(os.path.join(prep_path, '{}_ids_{}.npy'.format(label, self.name)), ids)\n        return X, y, ids","metadata":{"id":"lBDCMOzKrENS","execution":{"iopub.status.busy":"2022-04-12T20:50:31.108271Z","iopub.execute_input":"2022-04-12T20:50:31.109014Z","iopub.status.idle":"2022-04-12T20:50:31.126911Z","shell.execute_reply.started":"2022-04-12T20:50:31.108975Z","shell.execute_reply":"2022-04-12T20:50:31.126323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npreprocessor = InsightFace_Preprocessor(FACE_SIZE, faces=-1)                                                 # create face_recognition object\n\ntrain_X_INSIGHT, train_y_INSIGHT, INSIGHT_ids = preprocessor(train, train['class'].values, label='train')\n# test_X_INSIGHT, _, test_INSIGHT_ids = preprocessor(test)\nprint(f'Preprocessing resulted in {train_X_INSIGHT.shape[0]} images with {train_y_INSIGHT.shape[0]} labels.')","metadata":{"id":"oXQOdAogrENS","execution":{"iopub.status.busy":"2022-04-12T20:50:31.127901Z","iopub.execute_input":"2022-04-12T20:50:31.12875Z","iopub.status.idle":"2022-04-12T20:50:31.161567Z","shell.execute_reply.started":"2022-04-12T20:50:31.128704Z","shell.execute_reply":"2022-04-12T20:50:31.160632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_image_sequence(train_X_INSIGHT, n = 20)","metadata":{"id":"wnAO-1O_rENS","scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T20:50:31.163229Z","iopub.execute_input":"2022-04-12T20:50:31.163721Z","iopub.status.idle":"2022-04-12T20:50:33.674457Z","shell.execute_reply.started":"2022-04-12T20:50:31.163685Z","shell.execute_reply":"2022-04-12T20:50:33.673754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This preprocessor sees mixed success. It was able to expand the dataset to 110 images, meaning it was able to detect a lot more faces within the image. <br>\nIn one instance, however, it wasn't able to recognize a face at all, while `face_recognition` and even `HAAR` was able to find this face. <br>\nFor this dataset it might be a good idea to fallback on the face_recognition dataset when no face is found within the image. This would allow us to identify a lot more faces, whilst also eliminating the problematic images.","metadata":{"id":"LK7sQs24mgpy"}},{"cell_type":"markdown","source":"## 0.5. Store Preprocessed data (optional)\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n</div>","metadata":{"id":"HMVuCIjTrENT","papermill":{"duration":0.100995,"end_time":"2021-03-08T07:59:03.904684","exception":false,"start_time":"2021-03-08T07:59:03.803689","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# save preprocessed data\n# prep_path = './kaggle/working/prepped_data/'\n# if not os.path.exists(prep_path):\n#     os.mkdir(prep_path)\n    \n# np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n# np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)\n\n# load preprocessed data\n# prep_path = './kaggle/working/prepped_data/'\n# if not os.path.exists(prep_path):\n#     os.mkdir(prep_path)\n# train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n# train_y = np.load(os.path.join(prep_path, 'train_y.npy'))\n# test_X = np.load(os.path.join(prep_path, 'test_X.npy'))","metadata":{"id":"cA3yW4mLrENT","papermill":{"duration":0.109823,"end_time":"2021-03-08T07:59:04.11528","exception":false,"start_time":"2021-03-08T07:59:04.005457","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:33.675846Z","iopub.execute_input":"2022-04-12T20:50:33.676127Z","iopub.status.idle":"2022-04-12T20:50:33.680123Z","shell.execute_reply.started":"2022-04-12T20:50:33.676093Z","shell.execute_reply":"2022-04-12T20:50:33.67937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to rock!","metadata":{"id":"WI7Fnt-xrENT","papermill":{"duration":0.100101,"end_time":"2021-03-08T07:59:04.315571","exception":false,"start_time":"2021-03-08T07:59:04.21547","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Feature Representations\n## 1.0. Identify feature extractor\nOur example feature extractor doesn't actually do anything... It just returns the input:\n$$\n\\forall x : f(x) = x.\n$$\n\nIt does make for a good placeholder and baseclass ;).","metadata":{"id":"-ERsqXvorENT","papermill":{"duration":0.100212,"end_time":"2021-03-08T07:59:04.516059","exception":false,"start_time":"2021-03-08T07:59:04.415847","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class IdentityFeatureExtractor:\n    \"\"\"A simple function that returns the input\"\"\"\n    \n    def transform(self, X):\n        return X\n    \n    def __call__(self, X, label='train', preproc='HAAR'):\n        self.label = label\n        self.preproc = preproc\n        return self.transform(X)","metadata":{"id":"K5h2iRgyrENT","papermill":{"duration":0.108781,"end_time":"2021-03-08T07:59:04.725071","exception":false,"start_time":"2021-03-08T07:59:04.61629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:33.681364Z","iopub.execute_input":"2022-04-12T20:50:33.681631Z","iopub.status.idle":"2022-04-12T20:50:33.69183Z","shell.execute_reply.started":"2022-04-12T20:50:33.6816Z","shell.execute_reply":"2022-04-12T20:50:33.691006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Baseline 1: HOG feature extractor\nHOG stands for **H**istogram of **O**riented **G**radients. In this feature descriptor, the distribution of directions of gradients are used as features as they contain valueable information about corners and edges. \n\nFor this feature, we are going to use [scikit-image's hog descriptor](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html) which is popular for object detection algorithm of [Dalal and Triggs's paper](https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf).\n\nThere are couple of steps to generate HOG features:\n\n1. **Global Image Normalisation**: `block_norm` is used to indicate the normalisation process. We chose `L1` (Lasso regression) for our extractor. This step is optional\n2. **Computing the Gradient Image in x and y**: Calculation of horizontal and vertical gradients for each cell.\n3. **Computing Gradient Histograms**: Histograms are calculated per cell. In our extractor, we used 64 pixels (8x8) per cell(`pixels_per_cell=(8,8)`). `orientations` argument is to define the bin size of each histogram. Common behaviour is to take 9-bin histograms so that each bin represents 20 unsigned degrees.\n4. **Normalising Across Blocks:** After creating histogram for each cell, algorithm normalizes over blocks that consist of 4 cells(2x2) per block(`cells_per_block=(2,2)`). Normalizing over blocks help us to remove out the noise we have in the image such as illumination or shadowing by not losing important information.\n5. **Flattening Into a Feature Vector**: Concatanation of all vectors obtained from each block. \n\n[scikit-image hog extractor source code](https://github.com/scikit-image/scikit-image/blob/main/skimage/feature/_hog.py)","metadata":{"id":"U6WNz-03rENT","papermill":{"duration":0.134288,"end_time":"2021-03-08T07:59:04.959911","exception":false,"start_time":"2021-03-08T07:59:04.825623","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class HOGFeatureExtractor(IdentityFeatureExtractor):\n    \n    def __init__(self, orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2),\n                 visualize=True, multichannel=True, block_norm=\"L1\"):\n        self.params = {'orientations':orientations,\n                        'pixels_per_cell':pixels_per_cell,\n                        'cells_per_block':cells_per_block,\n                        'visualize':visualize,\n                        'multichannel':multichannel,\n                        'block_norm':block_norm\n                        }\n    \n    def features(self, img):\n        fd, hog_img = hog(img, **self.params)\n        return fd, hog_img\n    \n    def get_images(self):\n        return self.hog_img\n    \n    def transform(self, X):\n        features = [self.features(img) for img in X]\n        fd, self.hog_img = list(map(lambda x: x[0], features)), list(map(lambda x: x[1], features))\n        return np.array(fd)","metadata":{"id":"1kJogPjIrENU","papermill":{"duration":0.110122,"end_time":"2021-03-08T07:59:05.171171","exception":false,"start_time":"2021-03-08T07:59:05.061049","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:50:33.693516Z","iopub.execute_input":"2022-04-12T20:50:33.694203Z","iopub.status.idle":"2022-04-12T20:50:33.703417Z","shell.execute_reply.started":"2022-04-12T20:50:33.694166Z","shell.execute_reply":"2022-04-12T20:50:33.70256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhog_extr = HOGFeatureExtractor(block_norm='L1')\nhog_fd_train = hog_extr(train_X_HAAR)\nhog_img_train = hog_extr.get_images()\nprint(hog_fd_train.shape)\nplot_image_sequence(hog_img_train, n=30, imgs_per_row=10)","metadata":{"id":"rmPQ-wnbrENU","execution":{"iopub.status.busy":"2022-04-12T20:50:33.704622Z","iopub.execute_input":"2022-04-12T20:50:33.704928Z","iopub.status.idle":"2022-04-12T20:50:39.99155Z","shell.execute_reply.started":"2022-04-12T20:50:33.7049Z","shell.execute_reply":"2022-04-12T20:50:39.987251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.1. t-SNE Plots\nDo you have troubles to visualize your high-dimentional data? Well, worry no more! t-SNE is there for you! \n\n**t-Distributed Stochastic Neighbor Embedding (t-SNE)** is a dimensionality reduction technique used to represent high-dimensional dataset in a low-dimensional space of two or three dimensions by calculating a similarity measure between pairs of instances in the high dimensional space and in the low dimensional space.\n\n\n**Hyperparameters**\n\n*   `perplexity` is the # of nearest neighbors to indicate the size of circle which is used to lay out the gaussian distribution around the centered point. Values between 5 and 50 are mostly used values\n*   `n_components` indicates the dimension of the space to calculate the similarity\n*   `n_iter` indicates the # of iterations","metadata":{"id":"vNZJidZFrENU","papermill":{"duration":0.100377,"end_time":"2021-03-08T07:59:05.372401","exception":false,"start_time":"2021-03-08T07:59:05.272024","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_TSNE(features, target_y, n_components=2, verbose=1, perplexity=10, n_iter=10000, ttl='HoG'):\n    \"\"\"Function that plots the t-SNE for 2 components.\"\"\"\n    tsne = TSNE(n_components=n_components, verbose=verbose, perplexity=perplexity, n_iter=n_iter)\n    tsne_results = tsne.fit_transform(features)\n    x,y = tsne_results[:,0], tsne_results[:,1]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    if target_y is None:\n        ax.scatter(x, y)\n\n    else:\n        ax.scatter(x[target_y==0], y[target_y==0], c='r', marker='x', label='class 0')      \n        ax.scatter(x[target_y==1], y[target_y==1], c='g', marker='^', label='class 1')\n        ax.scatter(x[target_y==2], y[target_y==2], c='b', marker='o', label='class 2')\n        plt.legend()\n    plt.title(f't-SNE plot using {ttl}')\n    plt.ylabel(\"Component 1\")\n    plt.xlabel(\"Component 2\")\n    plt.show()","metadata":{"id":"I9dKXDq8rENU","execution":{"iopub.status.busy":"2022-04-12T20:50:39.993388Z","iopub.execute_input":"2022-04-12T20:50:39.993918Z","iopub.status.idle":"2022-04-12T20:50:40.00529Z","shell.execute_reply.started":"2022-04-12T20:50:39.993852Z","shell.execute_reply":"2022-04-12T20:50:40.004428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_TSNE(features=hog_fd_train, ttl='HoG', target_y=train_y_HAAR)","metadata":{"id":"rBzsSFTErENU","execution":{"iopub.status.busy":"2022-04-12T20:50:40.00662Z","iopub.execute_input":"2022-04-12T20:50:40.006861Z","iopub.status.idle":"2022-04-12T20:50:41.088822Z","shell.execute_reply.started":"2022-04-12T20:50:40.006822Z","shell.execute_reply":"2022-04-12T20:50:41.087958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Baseline 2: Scale Invariant Feature Transform (SIFT)\nSIFT, which is an abbreviation of Scale Invariant Feature Transform, is feature extraction method like HOG which transforms image content into local feature coordinates that are invariant to translation, scale and different other image transformations. An example application of SIFT can be found in this research  [(Basha, Padmaja & Balaji, 2019)](https://www.proquest.com/docview/2345487825/5934A9B0D3664BEEPQ/9) regarding bone fracture detection. A combination of the HAAR and SIFT transform techniques are used for improving the image quality such that appropriate fracture portion of the bone can be extracted for feature extraction.\n\nSome advantages of this method are the following: \n\n*   Locality: the features it detects are local and are hence robust to occlusion and clutter.\n*   Distinctiveness: the individual features the model extracts can be matched to a large dataset of objects.  \n*   Quantity: by using SIFT, one can extract a lot of features regardless the size of the objects. \n*   Lijstitem\n\n\nEfficiency: near real-time performance. ","metadata":{"id":"yOHFsULQrENV"}},{"cell_type":"code","source":"class SIFTFeatureExtractor(IdentityFeatureExtractor):\n    \n    def __init__(self, **params):\n        self.params = params\n        self.sift = cv2.SIFT_create()\n    \n    def features(self, img):\n        gray = cv2.cvtColor(np.uint8(img),cv2.COLOR_RGB2GRAY)        \n        kp, fd = self.sift.detectAndCompute(gray, None)\n        kp = np.array(kp)\n        kp_responses = np.array(list(map(lambda x: x.response, kp)))\n        idx = np.array(list(np.argpartition(kp_responses, -min(20, len(kp)))[-min(20, len(kp)):]))\n        if len(kp) != 0:\n            kp = kp[idx]\n            fd = fd[idx]\n        img = cv2.drawKeypoints(np.uint8(img),kp,gray)\n        return kp, fd, img\n    \n    def get_images(self):\n        return self.img\n    \n    def get_kp(self):\n        return self.kp\n    \n    def transform(self, X):\n        features = [self.features(img) for img in X]\n        self.kp, self.fd, self.img = zip(*features)\n        return np.array(self.fd)","metadata":{"id":"2OG5w5ZgrENV","execution":{"iopub.status.busy":"2022-04-12T20:50:41.09027Z","iopub.execute_input":"2022-04-12T20:50:41.09049Z","iopub.status.idle":"2022-04-12T20:50:41.0998Z","shell.execute_reply.started":"2022-04-12T20:50:41.090462Z","shell.execute_reply":"2022-04-12T20:50:41.098657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sift = SIFTFeatureExtractor()\nsift_fd = sift(train_X_INSIGHT)\nsift_img = sift.get_images()\nsift_kp = sift.get_kp()\nplot_image_sequence(sift_img, n=30, imgs_per_row=10)","metadata":{"id":"oj4q8aa-rENV","execution":{"iopub.status.busy":"2022-04-12T20:50:41.101435Z","iopub.execute_input":"2022-04-12T20:50:41.102119Z","iopub.status.idle":"2022-04-12T20:50:45.225268Z","shell.execute_reply.started":"2022-04-12T20:50:41.10208Z","shell.execute_reply":"2022-04-12T20:50:45.224351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10)\ncenters = []\nfor fd_ in sift_fd:\n    if fd_ is None:\n        centers.append(np.zeros((512,)))\n        continue\n    kmeans.fit(fd_)\n    kmeans.predict(fd_)\n    centers.append(kmeans.cluster_centers_.flatten())\ncenters = np.array(centers)","metadata":{"id":"9H62DgjBrENV","execution":{"iopub.status.busy":"2022-04-12T20:50:45.226569Z","iopub.execute_input":"2022-04-12T20:50:45.226988Z","iopub.status.idle":"2022-04-12T20:50:47.522957Z","shell.execute_reply.started":"2022-04-12T20:50:45.226948Z","shell.execute_reply":"2022-04-12T20:50:47.522021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.1 t-SNE Plot","metadata":{"id":"vb-IF4sJrHQn"}},{"cell_type":"code","source":"plot_TSNE(features=centers, ttl='SIFT descriptors', target_y=train_y_INSIGHT)","metadata":{"id":"zg29FqwvrKaK","execution":{"iopub.status.busy":"2022-04-12T20:50:47.524777Z","iopub.execute_input":"2022-04-12T20:50:47.52571Z","iopub.status.idle":"2022-04-12T20:50:48.80393Z","shell.execute_reply.started":"2022-04-12T20:50:47.52567Z","shell.execute_reply":"2022-04-12T20:50:48.803068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3. Baseline 3: FAST feature extractor\n`FAST` (or **F**eatures from **A**cceleated **S**egment **T**est) is a corner detection method. This feature extractor was made to have similar performance as `SIFT` or `ORB`. <br>\nHowever, as the name implied it was made to focus on computational efficiency, resulting in a lower cost of resources. This allows it to be more suitable toward processing video in real-time. <br>\nThis feature extractor will create a numpy array of 128 decimals, which are calculated based on every keypoint FAST is able to detect. <br>\nFor the purpose of this project, we have limited the maximum amount of keypoints to 20. These 20 keypoints had the highest reponse within the image and should most strongly characterize it. <br>\nTo actually use the feature descriptors within a machine learning context, the dimension will have to be reduced so that every image has an array of an equal size. This step is done by calculating the `kmeans` center on the descriptors of every image.","metadata":{"id":"y_fYc4M8rENV"}},{"cell_type":"code","source":"class FastFeatureExtractor(IdentityFeatureExtractor):\n    \n    def __init__(self, **params):\n        self.params = params\n        self.fast = cv2.FastFeatureDetector_create()\n        self.sift = cv2.SIFT_create()\n        \n    def features(self, img):\n        gray = cv2.cvtColor(np.uint8(img),cv2.COLOR_RGB2GRAY)\n        kp = np.array(list(self.fast.detect(gray, None)))\n        _, fd = self.sift.compute(gray, kp)\n        kp_responses = np.array(list(map(lambda x: x.response, kp)))\n        idx = np.array(list(np.argpartition(kp_responses, -min(20, len(kp)))[-min(20, len(kp)):]))\n        if len(kp) != 0:\n            kp = kp[idx]\n        if fd is None:\n            fd = []\n        img = cv2.drawKeypoints(np.uint8(img),kp,gray)\n        return kp, fd, img\n    \n    def get_images(self):\n        return self.img\n    \n    def get_keypoints(self):\n        return np.array(self.kp)\n    \n    def transform(self, X):\n        features = [self.features(img) for img in X]\n        self.kp, self.fd, self.img = zip(*features)\n        return np.array(self.fd)","metadata":{"id":"xtZevV7vrENV","execution":{"iopub.status.busy":"2022-04-12T20:50:48.805402Z","iopub.execute_input":"2022-04-12T20:50:48.80731Z","iopub.status.idle":"2022-04-12T20:50:48.816373Z","shell.execute_reply.started":"2022-04-12T20:50:48.807268Z","shell.execute_reply":"2022-04-12T20:50:48.815581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fast = FastFeatureExtractor()\nfast_fd = fast(train_X_INSIGHT)\nfast_img = fast.get_images()\nfast_kp = fast.get_keypoints()\nplot_image_sequence(fast_img, n=50, imgs_per_row=10)","metadata":{"id":"26a2CwW0rENW","execution":{"iopub.status.busy":"2022-04-12T20:50:48.817758Z","iopub.execute_input":"2022-04-12T20:50:48.818056Z","iopub.status.idle":"2022-04-12T20:50:55.386206Z","shell.execute_reply.started":"2022-04-12T20:50:48.817968Z","shell.execute_reply":"2022-04-12T20:50:55.38516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=2, init='k-means++', n_init=10)\ncenters = []\nfor i, fd_ in enumerate(fast_fd):\n    if len(fd_) == 0:\n        fd_ = sift_fd[i]\n    if fd_ is None:\n        centers.append(np.zeros((256,)))\n        continue\n    kmeans.fit(fd_)\n    kmeans.predict(fd_)\n#     print(kmeans.cluster_centers_.flatten().shape)\n    centers.append(kmeans.cluster_centers_.flatten())\ncenters = np.array(centers)","metadata":{"id":"wS23CXkyrENW","execution":{"iopub.status.busy":"2022-04-12T20:50:55.387699Z","iopub.execute_input":"2022-04-12T20:50:55.388631Z","iopub.status.idle":"2022-04-12T20:51:57.532532Z","shell.execute_reply.started":"2022-04-12T20:50:55.388584Z","shell.execute_reply":"2022-04-12T20:51:57.531701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3.1 t-SNE Plot","metadata":{"id":"VX9hxYbJrN60"}},{"cell_type":"code","source":"plot_TSNE(features=centers, ttl='FAST descriptors', target_y=train_y_INSIGHT)","metadata":{"id":"Bnf7bggUrQT9","execution":{"iopub.status.busy":"2022-04-12T20:51:57.533941Z","iopub.execute_input":"2022-04-12T20:51:57.534606Z","iopub.status.idle":"2022-04-12T20:51:58.271601Z","shell.execute_reply.started":"2022-04-12T20:51:57.534555Z","shell.execute_reply":"2022-04-12T20:51:58.270761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4. Baseline 4: PCA feature extractor\nAnother commonly used feature extraction method is the Principle Component Analysis, PCA in short. PCA aims to find the eigenvectors of a covariance matrix with the highest eigenvalues and hence uses them to project the input data into a new space of equal or less dimensions. In practice, the method converts a matrix of X features into a new dataset of, ideally, < X features. In other words, it reduces the number of features by constructing a new, smaller number variables that capture a remarkable portion of the information found in the original features. An example of this method can be found in the [paper of Ma, Miao and Zhang (2013)](https://www.proquest.com/docview/1776429183/FC068F7F84A544D0PQ/8) where they used PCA in order to utilize the global features to efficiently classify vehicles. ","metadata":{"id":"TYIRdk_mrENW"}},{"cell_type":"code","source":"def make_flat_grays(images):\n    Grays = []\n    n_samples = int(np.shape(images)[0])\n    for i in range(0,n_samples):\n            # take the ith image from the input\n            image = images[i,:,:,:]\n            # set to grayscale\n            gray_image = cv2.cvtColor(image.astype('uint8'), cv2.COLOR_RGB2GRAY)\n            # resize to 1 dimension\n            flat_gray_image = np.reshape(gray_image, -1)\n            # stack all resized images\n            Grays.append(flat_gray_image)\n    return Grays\n\nclass PCAFeatureExtractor(IdentityFeatureExtractor):\n    \n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.processing = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=n_components))])\n        \n    def transf(self, images):\n        # preprocess the data: \n        Grays = make_flat_grays(images)\n        Grays = np.array(Grays)\n        Grays = Grays.T\n        \n        # perform the PCA\n        self.eigenfaces = self.processing.fit_transform(Grays)\n        \n        return self.eigenfaces\n    \n    def inv_transform(self, eigenfaces):\n        self.reconstructed_faces = self.processing.named_steps['pca'].inverse_transform(eigenfaces)\n        return self.reconstructed_faces \n    \n    def create_pca_object(self, images):\n        Grays = make_flat_grays(images)\n        Grays = np.array(Grays).T\n        pca = PCA(n_components = self.n_components, whiten = True)\n        pca.fit(Grays)\n        return pca","metadata":{"id":"9zmKJST7rENW","papermill":{"duration":0.111032,"end_time":"2021-03-08T07:59:06.191215","exception":false,"start_time":"2021-03-08T07:59:06.080183","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:51:58.273057Z","iopub.execute_input":"2022-04-12T20:51:58.273905Z","iopub.status.idle":"2022-04-12T20:51:58.283291Z","shell.execute_reply.started":"2022-04-12T20:51:58.273869Z","shell.execute_reply":"2022-04-12T20:51:58.282541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4.1. Eigenface Plots\nWhen performing PCA on the faces in the test set, the principal components that we extract are called Eigenfaces instead of Eigenvectors. Each of these Eigenfaces represents a basic, standardized form of a face which displays one or more features of a face. Any existing face can be created by combining these Eigenfaces in a different way and it does not take many of these Eigenfaces to obtain a nice face approximation. Therefore, Eigenfaces are frequently used for face recognition purposes, for example in [this](https://sites.cs.ucsb.edu/~mturk/Papers/mturk-CVPR91.pdf) and [this](https://www.cec.uchile.cl/~aabdie/jruizd/papers/afss2002b.pdf) research.\n\nThe first Eigenface is the face that represents the greatest variance among all the faces in the test set. Therefore, it is a very general representation of a face as can be seen in the images below. The second face explains less variance, the third even less than the second and so on. The last Eigenfaces can even be left out as they explain almost no variance.","metadata":{"id":"b2VzZDO-rENX","papermill":{"duration":0.100881,"end_time":"2021-03-08T07:59:06.392861","exception":false,"start_time":"2021-03-08T07:59:06.29198","status":"completed"},"tags":[]}},{"cell_type":"code","source":"n_components = 39\nimages_gray = make_flat_grays(train_X_HAAR[train_y_HAAR == 2])\nmean_face = np.mean(images_gray, axis=0)\nmean_face = np.reshape(mean_face,(FACE_SIZE[0],FACE_SIZE[1]))\nstd_face = np.std(images_gray, axis = 0)\nstd_face = np.reshape(std_face,(FACE_SIZE[0],FACE_SIZE[1]))\nExtractor = PCAFeatureExtractor(n_components = n_components)\n\nEigenfaces_flat = Extractor.transf(train_X_HAAR[train_y_HAAR == 2])\nEigenfaces_as_image = np.empty((n_components,FACE_SIZE[0],FACE_SIZE[1],1))\nfor i in range(0,n_components):\n    image = np.reshape(Eigenfaces_flat[:,i], (FACE_SIZE[0],FACE_SIZE[1]))\n    image = np.expand_dims(image, axis = 0)\n    Eigenfaces_as_image[i,0:FACE_SIZE[0],0:FACE_SIZE[1],0] = image\n\nprint(Eigenfaces_as_image.shape)\nplot_gray_image_sequence(Eigenfaces_as_image, n=n_components, imgs_per_row=10)","metadata":{"id":"4z4mpO8urENX","outputId":"fbb76e7c-e826-4dcd-9ee2-9f9a2fb7f180","execution":{"iopub.status.busy":"2022-04-12T20:51:58.284487Z","iopub.execute_input":"2022-04-12T20:51:58.285258Z","iopub.status.idle":"2022-04-12T20:52:03.458321Z","shell.execute_reply.started":"2022-04-12T20:51:58.28521Z","shell.execute_reply":"2022-04-12T20:52:03.457535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstructed_imgs = Extractor.inv_transform(Eigenfaces_flat)\nreconstructed_imgs = reconstructed_imgs\nrec_imgs_as_image = np.empty((np.shape(reconstructed_imgs)[0],FACE_SIZE[0],FACE_SIZE[1],1))\nprint(np.shape(reconstructed_imgs))\nfor i in range(0,np.shape(reconstructed_imgs)[1]):\n    reconstructed_image = np.reshape(reconstructed_imgs[:,i], (FACE_SIZE[0],FACE_SIZE[1]))\n    reconstructed_image = reconstructed_image*std_face + mean_face\n    reconstructed_image = np.expand_dims(reconstructed_image, axis = 0)\n    rec_imgs_as_image[i,0:FACE_SIZE[0],0:FACE_SIZE[1],0] = reconstructed_image\nplot_gray_image_sequence(rec_imgs_as_image, n=np.shape(reconstructed_imgs)[1], imgs_per_row=10)","metadata":{"id":"54gBqXmSrENX","execution":{"iopub.status.busy":"2022-04-12T20:52:03.459739Z","iopub.execute_input":"2022-04-12T20:52:03.460476Z","iopub.status.idle":"2022-04-12T20:52:08.750805Z","shell.execute_reply.started":"2022-04-12T20:52:03.460428Z","shell.execute_reply":"2022-04-12T20:52:08.750129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4.2. Feature Space Plots\n\nThe following visualization displays the images projected on and reconstructed from the first two eigenfaces. The images that are close to each other are the more similar ones whereas the dissimilar images are far apart in the graph. ","metadata":{"id":"2w5D1fTorENX","papermill":{"duration":0.101263,"end_time":"2021-03-08T07:59:06.797448","exception":false,"start_time":"2021-03-08T07:59:06.696185","status":"completed"},"tags":[]}},{"cell_type":"code","source":"extractor = PCAFeatureExtractor(n_components = 39)\ntransformed_faces = extractor.transf(train_X_HAAR)\nprint(transformed_faces.shape)\n#faces_inv_proj_2 = reconstructed_imgs\nreconstructed_faces = np.reshape(extractor.inv_transform(transformed_faces),(104,100,100))\nfig,ax = plt.subplots()\nax.scatter(transformed_faces[:104,0], transformed_faces[:104,1])\nfor x0, y0, rec_face in zip(transformed_faces[:,0], transformed_faces[:,1], reconstructed_faces):\n    ab = AnnotationBbox(OffsetImage(mean_face + std_face * rec_face, zoom=0.3, cmap='gray'), (x0, y0), frameon=False)\n    ax.add_artist(ab)\nax.set_title('Projecting the images in the 2D eigenspace')\nax.set_xlabel('Eigenface 1')\nax.set_ylabel('Eigenface 2')\nfig.set_figheight(10)\nfig.set_figwidth(15)","metadata":{"id":"estxvJXZrENY","execution":{"iopub.status.busy":"2022-04-12T20:52:10.121259Z","iopub.execute_input":"2022-04-12T20:52:10.122069Z","iopub.status.idle":"2022-04-12T20:52:11.409045Z","shell.execute_reply.started":"2022-04-12T20:52:10.122033Z","shell.execute_reply":"2022-04-12T20:52:11.408462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5. Baseline 5: Facial encodings\nThis feature extractor is made using the `face_recognition` package that has earlier been used for preprocessing. <br>\nIn this section the same package is used to extract the facial encodings in an image. Because every image has been cropped to only show the faces, this function is (theoretically) possible with every preprocessor. <br>\nHowever in testing it becomes clear that this is not always the case. This feature extractor returns an empty array when ran on blurry images and sometimes even on clear images when they are not cropped in the same way the face_recognition package expects them to be. ","metadata":{"id":"kNs7oAjprENY"}},{"cell_type":"code","source":"class FEncodings_FeatureExtractor(IdentityFeatureExtractor):\n    \"\"\"Use facial landmarks as computed by face_recognition\"\"\"\n    def __init__(self):\n        self.prep_path = \"/kaggle/working/prepped_data/2. Features/Facial Encodings\"\n\n    def extract(self, img):\n        img_tmp = cv2.cvtColor(np.uint8(img), cv2.COLOR_BGR2RGB)\n        return fr.face_encodings(img_tmp, model='hog')\n    \n    def check_prep(self):\n        pathlib.Path(self.prep_path).mkdir(parents=True, exist_ok=True)\n        \n        bool_FE = pathlib.Path(os.path.join(self.prep_path, '{}_{}_FE.npy'.format(self.label, self.preproc))).exists()\n        return bool_FE\n        \n    def transform(self, X):          \n        if self.check_prep():\n            print('Loading prepped data...')\n            encodings = np.load(os.path.join(self.prep_path, '{}_{}_FE.npy'.format(self.label, self.preproc)))\n        else:\n            encodings = []\n            for img in tqdm(X):\n                fe = self.extract(img)\n                if len(fe) == 0:\n                    encodings.extend(np.zeros((1,128)))\n                else:\n                    encodings.extend(fe)\n            encodings = np.array(encodings)\n            np.save(os.path.join(self.prep_path, '{}_{}_FE.npy'.format(self.label, self.preproc)), encodings)\n        return encodings","metadata":{"id":"8ofsMq39rENY","execution":{"iopub.status.busy":"2022-04-12T20:52:11.410222Z","iopub.execute_input":"2022-04-12T20:52:11.410568Z","iopub.status.idle":"2022-04-12T20:52:11.41882Z","shell.execute_reply.started":"2022-04-12T20:52:11.410538Z","shell.execute_reply":"2022-04-12T20:52:11.417944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nFEncodings = FEncodings_FeatureExtractor()\nencodings = FEncodings(train_X_FACEREC, 'train', 'FACEREC')","metadata":{"id":"A3xpRBR8rENY","execution":{"iopub.status.busy":"2022-04-12T20:52:11.420078Z","iopub.execute_input":"2022-04-12T20:52:11.420293Z","iopub.status.idle":"2022-04-12T20:52:11.436119Z","shell.execute_reply.started":"2022-04-12T20:52:11.420267Z","shell.execute_reply":"2022-04-12T20:52:11.435249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2. Discussion\n...","metadata":{"id":"BWbgYEplrENY","papermill":{"duration":0.100596,"end_time":"2021-03-08T07:59:05.775686","exception":false,"start_time":"2021-03-08T07:59:05.67509","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 2. Cleaning the dataset","metadata":{"id":"E_X9jAEcrENY"}},{"cell_type":"markdown","source":"### 2.1 Remove most dissimilar faces from dataset\nAn important step is the cleaning of the dataset. <br>\nIn the previous section surround preprocessing, the decision was made to allow the face detection to detect more than one face. This means that our dataset has been expanded, but now we are faced with a lot of images that are either <u>irrelevant</u> or <u>mislabeled</u>. As such this function exists to scale the dataset back to its original shape. <br><br>\nTo get the best possible results, there exist different functions for training sets and testing sets. <br><br>\nIn the scenario that a `training` dataset is passed on to the function, it will iterate over every picture and calculate the cosine distance with every image of the same class after which this value is averaged. Afterwards the dataset will go over every row for which the image id appears multiple times and will only keep the image with the highest similarity. The `cosine similarity` is calculated by comparing the features that were passed on, however, if the feature is equal to an array consisting only of zeroes, it will compare the similarities between the images directly.<br><br>\nFor the `testing` dataset this is done in a similar way, but because the class label is not known a similarity score is calculated for every class. After which it will keep the image in which the highest similarity was attained, regardless of the class.\n","metadata":{"id":"c1_BBg5LrENZ"}},{"cell_type":"code","source":"class CleanTrainingSet():\n    def __init__(self, train_X, train_y, ids, features):\n        self.train_X = train_X\n        self.train_y = train_y\n        self.ids = ids\n        self.features = features\n        self.prep_path = \"/kaggle/working/prepped_data/3. Clean Data/\"\n        \n    def create_df(self):\n        self.data = pd.DataFrame({'id': self.ids})\n        if self.train_y is not None:\n            self.data['class'] = list(self.train_y)\n        self.data['img'] = list(self.train_X)\n        self.data['feature'] = list(self.features)\n        return self.data\n        \n    def cosine_sim_train(self, row):\n        sim = []\n        if (row['feature'] == 0).all():\n            for class_feature in self.data[self.data['class'] == row['class']]['img'].to_numpy():            \n                tmp = (distance.cosine(class_feature.flatten(), row['img'].flatten()) - 1) * (-1)\n                sim.append(tmp)\n            sim = np.array(sim)\n            return np.mean(sim[~np.isnan(sim)])\n        \n        for class_feature in self.data[self.data['class'] == row['class']]['feature'].to_numpy():\n            tmp = (distance.cosine(class_feature, row['feature']) - 1) * (-1)\n            sim.append(tmp)\n        sim = np.array(sim)\n        tmp = np.mean(sim[~np.isnan(sim)])\n        return 0 if np.isnan(tmp) else tmp\n    \n    def cosine_sim_test(self, row, class_id):\n        sim = []\n        if (row['feature'] == 0).all():\n            for class_feature in self.train_data[self.train_data['class'] == class_id]['img'].to_numpy():            \n                tmp = (distance.cosine(class_feature.flatten(), row['img'].flatten()) - 1) * (-1)\n                sim.append(tmp)\n            sim = np.array(sim)\n            return np.mean(sim[~np.isnan(sim)])\n        \n        for class_feature in self.train_data[self.train_data['class'] == class_id]['feature'].to_numpy():\n            tmp = (distance.cosine(class_feature, row['feature']) - 1) * (-1)\n            sim.append(tmp)\n        sim = np.array(sim)\n        return np.mean(sim[~np.isnan(sim)])\n    \n    def calc_sim(self):\n        if self.train_y is not None:\n            self.data['cos_sim'] = self.data.apply(self.cosine_sim_train, axis=1)\n        else:\n            self.data['cos_sim_0'] = self.data.apply(self.cosine_sim_test, args=(0,), axis=1)\n            self.data['cos_sim_1'] = self.data.apply(self.cosine_sim_test, args=(1,), axis=1)\n            self.data['cos_sim_2'] = self.data.apply(self.cosine_sim_test, args=(2,), axis=1)\n            self.data['cos_sim'] = self.data[['cos_sim_0', 'cos_sim_1', 'cos_sim_2']].max(axis=1)\n        return self.data\n    \n    \n    def drop_dissimilar(self):\n        amount = self.data.groupby('id').agg({'id':'count'})\n        idx = list(amount[amount['id'] > 1].index)\n        \n        for ind in idx:\n            if (self.data[self.data['id'] == ind]['cos_sim'] == 0).all():\n                0\n            max_sim = self.data[self.data['id'] == ind]['cos_sim'].max()\n            self.data.drop(self.data[np.logical_and(self.data['id'] == ind, self.data['cos_sim'] < max_sim)].index,\n                           inplace=True)\n        self.data.reset_index(drop=True, inplace=True)\n        return self.data\n    \n    def replace_black(self, fallback_X, fallback_ids):\n        for i, row in self.data.iterrows():\n            if not np.count_nonzero(row['img']):\n                arg = np.argwhere(fallback_ids == row['id']).flatten()[0]\n                if np.count_nonzero(fallback_X[arg]):\n                    self.data.at[i, 'img'] = fallback_X[arg]\n        return\n                    \n    def load_training(self, train_X, train_y, feature):\n        self.train_data = pd.DataFrame({'class': train_y, 'img': list(train_X), 'feature': list(feature)})\n        return self.train_data\n        \n        \n    def get_data(self):\n        return self.data\n    \n    def check_prep(self, y):\n        pathlib.Path(self.prep_path).mkdir(parents=True, exist_ok=True)\n        \n        bool_X = pathlib.Path(os.path.join(self.prep_path, '{}_{}_{}_X_clean.npy'.format(self.label, self.feature_name,\n                                                                                   self.preproc))).exists()\n        if y is None:\n            bool_y = pathlib.Path(os.path.join(self.prep_path, '{}_{}_{}_y_clean.npy'.format(self.label, self.feature_name,\n                                                                                   self.preproc))).exists()\n        else:\n            bool_y = True\n        return (bool_X and bool_y)\n    \n    def get_class(self):\n        y = np.load(os.path.join(self.prep_path, '{}_{}_{}_y_clean.npy'.format(self.label, self.feature_name,\n                                                                                   self.preproc)))\n        return y\n        \n    def clean(self, train_X=None, train_y=None, train_features=None, fallback_X=None, fallback_ids=None,\n              label='train', feature='hog', preproc='HAAR'):\n        self.label = label\n        self.feature_name = feature\n        self.preproc = preproc\n        \n        if self.check_prep(train_y):\n            print('Loading cleaned dataset...')\n            X = np.load(os.path.join(self.prep_path, '{}_{}_{}_X_clean.npy'.format(self.label, self.feature_name,\n                                                                                   self.preproc)), allow_pickle=True)\n        else:\n            self.create_df()\n            if train_X is not None and train_y is not None and train_features is not None:\n                self.load_training(train_X, train_y, train_features)\n            self.calc_sim()\n            self.drop_dissimilar()  \n            if fallback_X is not None and fallback_ids is not None:\n                self.replace_black(fallback_X, fallback_ids)\n\n            X = self.data['img'].to_numpy()\n            np.save(os.path.join(self.prep_path, '{}_{}_{}_X_clean.npy'.format(self.label, self.feature_name,\n                                                                               self.preproc)), X)\n            if train_y is None:\n                y = self.data['class'].to_numpy()            \n                np.save(os.path.join(self.prep_path, '{}_{}_{}_y_clean.npy'.format(self.label, self.feature_name,\n                                                                               self.preproc)), y)\n        return X","metadata":{"id":"gN_VnQWKrENZ","execution":{"iopub.status.busy":"2022-04-12T20:52:11.437686Z","iopub.execute_input":"2022-04-12T20:52:11.43795Z","iopub.status.idle":"2022-04-12T20:52:11.468413Z","shell.execute_reply.started":"2022-04-12T20:52:11.437916Z","shell.execute_reply":"2022-04-12T20:52:11.467386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning HAAR dataset","metadata":{"id":"jGIxilaCrENZ"}},{"cell_type":"code","source":"cleaner = CleanTrainingSet(train_X_HAAR, train_y_HAAR, HAAR_ids, hog_fd_train)\ntrain_X_HAAR = cleaner.clean(label='train', feature='hog', preproc='HAAR')\ntrain_y_HAAR = cleaner.get_class()\ntrain_X_HAAR.shape","metadata":{"id":"9SFGabNxrENa","execution":{"iopub.status.busy":"2022-04-12T20:52:11.469848Z","iopub.execute_input":"2022-04-12T20:52:11.470227Z","iopub.status.idle":"2022-04-12T20:52:11.497981Z","shell.execute_reply.started":"2022-04-12T20:52:11.470192Z","shell.execute_reply":"2022-04-12T20:52:11.497126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_image_sequence(train_X_HAAR, n=train_X_HAAR.shape[0])","metadata":{"id":"5p5pu5DnrENa","execution":{"iopub.status.busy":"2022-04-12T20:52:11.49928Z","iopub.execute_input":"2022-04-12T20:52:11.49986Z","iopub.status.idle":"2022-04-12T20:52:20.861876Z","shell.execute_reply.started":"2022-04-12T20:52:11.499822Z","shell.execute_reply":"2022-04-12T20:52:20.860377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning FaceRec dataset","metadata":{"id":"KEDKkLKnrENa"}},{"cell_type":"code","source":"cleaner = CleanTrainingSet(train_X_FACEREC, train_y_FACEREC, FACEREC_ids, encodings)\ntrain_X_FACEREC = cleaner.clean(label='train', feature='FE', preproc='FACEREC')\ntrain_y_FACEREC = cleaner.get_class()\ntrain_X_FACEREC.shape","metadata":{"id":"3mcLj2SCrENa","execution":{"iopub.status.busy":"2022-04-12T20:52:20.863274Z","iopub.execute_input":"2022-04-12T20:52:20.863626Z","iopub.status.idle":"2022-04-12T20:52:20.883852Z","shell.execute_reply.started":"2022-04-12T20:52:20.863589Z","shell.execute_reply":"2022-04-12T20:52:20.883096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_image_sequence(train_X_FACEREC, n=train_X_FACEREC.shape[0])","metadata":{"id":"6ph8Lm5GrENa","execution":{"iopub.status.busy":"2022-04-12T20:52:20.885291Z","iopub.execute_input":"2022-04-12T20:52:20.886225Z","iopub.status.idle":"2022-04-12T20:52:30.258029Z","shell.execute_reply.started":"2022-04-12T20:52:20.886187Z","shell.execute_reply":"2022-04-12T20:52:30.256633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning InsightFace dataset","metadata":{"id":"i7ruF1hIrENa"}},{"cell_type":"code","source":"feature_extractor = FEncodings_FeatureExtractor()\ncleaner = CleanTrainingSet(train_X_INSIGHT, train_y_INSIGHT, INSIGHT_ids, \n                           feature_extractor(train_X_INSIGHT, 'train', 'INSIGHT'))\ntrain_X_INSIGHT = cleaner.clean(fallback_X=train_X_FACEREC, fallback_ids=FACEREC_ids, label='train', feature='FE', preproc='INSIGHT')\ntrain_y_INSIGHT = cleaner.get_class()\ntrain_X_INSIGHT.shape","metadata":{"id":"bbtVj_jZrENa","execution":{"iopub.status.busy":"2022-04-12T20:52:30.260092Z","iopub.execute_input":"2022-04-12T20:52:30.26069Z","iopub.status.idle":"2022-04-12T20:52:42.614085Z","shell.execute_reply.started":"2022-04-12T20:52:30.26065Z","shell.execute_reply":"2022-04-12T20:52:42.613199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_image_sequence(train_X_INSIGHT, n=train_X_INSIGHT.shape[0])","metadata":{"id":"dulkk6MurENb","execution":{"iopub.status.busy":"2022-04-12T20:52:42.615362Z","iopub.execute_input":"2022-04-12T20:52:42.615601Z","iopub.status.idle":"2022-04-12T20:52:52.04596Z","shell.execute_reply.started":"2022-04-12T20:52:42.615572Z","shell.execute_reply":"2022-04-12T20:52:52.044989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Removing fully black images\nWhen visually inspecting the training dataset, it could be observed that there was one fully black image on which no face could be read. <br>\nTo make sure that fully black images don't skew the training, they are removed from the dataset.","metadata":{"id":"RiomD-C0rENZ"}},{"cell_type":"code","source":"def remove_blacks(train_X, train_y):\n    for i, row in enumerate(train_X):\n        if not np.count_nonzero(row):\n            train_X = np.delete(train_X, i)\n            train_y = np.delete(train_y, i)\n    return train_X, train_y","metadata":{"id":"8qLqS4YurENZ","execution":{"iopub.status.busy":"2022-04-12T20:52:52.047947Z","iopub.execute_input":"2022-04-12T20:52:52.048215Z","iopub.status.idle":"2022-04-12T20:52:52.053925Z","shell.execute_reply.started":"2022-04-12T20:52:52.048185Z","shell.execute_reply":"2022-04-12T20:52:52.053187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X_FACEREC, train_y_FACEREC = remove_blacks(train_X_FACEREC, train_y_FACEREC)\ntrain_X_FACEREC.shape","metadata":{"id":"nXwXwl_yMLnw","execution":{"iopub.status.busy":"2022-04-12T20:52:52.055176Z","iopub.execute_input":"2022-04-12T20:52:52.055441Z","iopub.status.idle":"2022-04-12T20:52:52.072633Z","shell.execute_reply.started":"2022-04-12T20:52:52.055372Z","shell.execute_reply":"2022-04-12T20:52:52.071894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Augmenting the training set\n","metadata":{"id":"6izzWyLF84vW"}},{"cell_type":"markdown","source":"Another way of improving our results was done by augmenting the dataset. In this instance it will take the preprocessed and cleaned data, with the black images removed. After this, it will iterate over the entire dataset and for every image it will create 7 <u>alternative</u> images using the following methods:\n1. `Flipping` the image horizontally\n2. Applying a `Gaussian blur` with sigma 0.50\n3. Applying a `Gaussian blur` with sigma 1.50\n4. `Lighten` the picture by multiplying by 1.50\n5. `Darken` the picture by multiplying by 0.50\n6. Add `\"Salt and Pepper\"` noise by changing 2% of the total pixels\n7. Add `\"Salt and Pepper\"` noise by changing 5% of the total pixels","metadata":{"id":"hP5QJ2eaptHR"}},{"cell_type":"code","source":"class AugmentDataset():\n    def __init__(self):\n        self.fliplr = ia.augmenters.Fliplr(1)\n        self.blur050 = ia.augmenters.GaussianBlur(sigma=0.50)\n        self.blur100 = ia.augmenters.GaussianBlur(sigma=1.5)\n        self.lighten = ia.augmenters.Multiply(1.5)\n        self.darken = ia.augmenters.Multiply(0.5)\n        self.noise2 = ia.augmenters.SaltAndPepper(0.02)\n        self.noise5 = ia.augmenters.SaltAndPepper(0.05)\n      \n    def augment(self, img):\n        new_train_X = []\n        new_train_X.append(np.uint8(img))\n        new_train_X.append(self.fliplr(image=np.uint8(img)))\n        new_train_X.append(self.blur050(image=np.uint8(img)))\n        new_train_X.append(self.blur100(image=np.uint8(img)))\n        new_train_X.append(self.lighten(image=np.uint8(img)))\n        new_train_X.append(self.darken(image=np.uint8(img)))\n        new_train_X.append(self.noise2(image=np.uint8(img)))\n        new_train_X.append(self.noise5(image=np.uint8(img)))\n        return new_train_X\n    \n    def __call__(self, X, y):\n        new_X = []\n        new_y = []\n        for i, img in enumerate(X):\n            new_X.extend(self.augment(img))\n            new_y.extend([y[i] for _ in range(8)])\n\n        tmp = np.empty(len(new_X), dtype=object)\n        tmp[:] = new_X\n        new_X = tmp\n        return np.array(new_X), np.array(new_y)","metadata":{"id":"XMo5Fmt39_yY","execution":{"iopub.status.busy":"2022-04-12T20:52:52.074188Z","iopub.execute_input":"2022-04-12T20:52:52.074415Z","iopub.status.idle":"2022-04-12T20:52:52.084792Z","shell.execute_reply.started":"2022-04-12T20:52:52.074387Z","shell.execute_reply":"2022-04-12T20:52:52.084204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmenter = AugmentDataset()\nnew_train_X, new_train_y = augmenter(train_X_FACEREC,train_y_FACEREC)\nprint(new_train_X.shape, new_train_y.shape)","metadata":{"id":"lXGdX6ZsAR6K","execution":{"iopub.status.busy":"2022-04-12T20:52:52.085742Z","iopub.execute_input":"2022-04-12T20:52:52.085949Z","iopub.status.idle":"2022-04-12T20:52:52.464183Z","shell.execute_reply.started":"2022-04-12T20:52:52.085922Z","shell.execute_reply":"2022-04-12T20:52:52.46339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Evaluation Metrics","metadata":{"id":"XqODy1NOrENb","papermill":{"duration":0.10088,"end_time":"2021-03-08T07:59:07.406787","exception":false,"start_time":"2021-03-08T07:59:07.305907","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self):\n        return\n    \n    def get_metrics(self):\n        metric_dict = {'Accuracy': accuracy_score(self.train_y, self.train_y_star),\n                      'Recall': recall_score(self.train_y, self.train_y_star, average=\"weighted\"),\n                      'Precision': precision_score(self.train_y, self.train_y_star, average=\"weighted\"),\n                      'F1': f1_score(self.train_y, self.train_y_star, average=\"weighted\")}\n        return metric_dict\n    \n    def __call__(self, train_y, train_y_star):\n        self.train_y = train_y\n        self.train_y_star = train_y_star\n        return self.get_metrics()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:52:52.465422Z","iopub.execute_input":"2022-04-12T20:52:52.46578Z","iopub.status.idle":"2022-04-12T20:52:52.471538Z","shell.execute_reply.started":"2022-04-12T20:52:52.46575Z","shell.execute_reply":"2022-04-12T20:52:52.470617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.0. Accuracy\nThis metric is the ratio of the correctly predicted observation to the total number of observations. In our case this will be the number of correctly classified images divided by the total number of the images. ","metadata":{"id":"XqODy1NOrENb","papermill":{"duration":0.10088,"end_time":"2021-03-08T07:59:07.406787","exception":false,"start_time":"2021-03-08T07:59:07.305907","status":"completed"},"tags":[]}},{"cell_type":"raw","source":"from sklearn.metrics import accuracy_score","metadata":{"id":"k-rTjePurENb","papermill":{"duration":1.180116,"end_time":"2021-03-08T07:59:08.688561","exception":false,"start_time":"2021-03-08T07:59:07.508445","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 2.1. Recall\nRecall is the number of correctly predicted positive observations relative to the number of observations in the actual class. In other words, for all the images we have of person X, for how many of them did we correctly classify them as person X?","metadata":{"id":"fhqQiaTgrENb"}},{"cell_type":"raw","source":"from sklearn.metrics import recall_score","metadata":{"id":"CXC3BpL0rENb"}},{"cell_type":"markdown","source":"## 2.2. Precision\nThe ratio of correctly predicted positive observations to the total predicted positive observations. In our case, for all the images we classified as person X, how many of them were person X?","metadata":{"id":"pwmOAJDLrENb"}},{"cell_type":"raw","source":"from sklearn.metrics import precision_score","metadata":{"id":"M1LyAZIBrENb"}},{"cell_type":"markdown","source":"## 2.3. F1 score\nIt is the weighted average of the precision and recall metric. ","metadata":{"id":"vUDjzl56rENc"}},{"cell_type":"raw","source":"from sklearn.metrics import f1_score","metadata":{"id":"uR7yvZcKrENc"}},{"cell_type":"markdown","source":"# 3. Classifiers","metadata":{"id":"rjJe3WQerENc","papermill":{"duration":0.103749,"end_time":"2021-03-08T07:59:08.894358","exception":false,"start_time":"2021-03-08T07:59:08.790609","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class RandomClassificationModel:\n    \"\"\"Random classifier, draws a random sample based on class distribution observed \n    during training.\"\"\"\n    \n    def fit(self, X, y):\n        \"\"\"Adjusts the class ratio instance variable to the one observed in y. \n\n        Parameters\n        ----------\n        X : tensor\n            Training set\n        y : array\n            Training set labels\n\n        Returns\n        -------\n        self : RandomClassificationModel\n        \"\"\"\n        \n        self.classes, self.class_ratio = np.unique(y, return_counts=True)\n        self.class_ratio = self.class_ratio / self.class_ratio.sum()\n        return self\n        \n    def predict(self, X):\n        \"\"\"Samples labels for the input data. \n\n        Parameters\n        ----------\n        X : tensor\n            dataset\n            \n        Returns\n        -------\n        y_star : array\n            'Predicted' labels\n        \"\"\"\n\n        np.random.seed(0)\n        return np.random.choice(self.classes, size = X.shape[0], p=self.class_ratio)\n    \n    def __call__(self, X):\n        return self.predict(X)\n    ","metadata":{"id":"zXDdi7pxrENc","execution":{"iopub.status.busy":"2022-04-12T20:52:52.473007Z","iopub.execute_input":"2022-04-12T20:52:52.473266Z","iopub.status.idle":"2022-04-12T20:52:52.490334Z","shell.execute_reply.started":"2022-04-12T20:52:52.473235Z","shell.execute_reply":"2022-04-12T20:52:52.489432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Baseline 1: MultiClass SVM\nThe Support Vector Machine algorithm tries to find a line that maximizes the separation between a 2-class data set of 2D space points. The data points that are the closest to this line, in other words, within a minimum distance are called the support vectors. Normally, SVM does not support multiclass classification, namely when an instance needs to be classified as only one out of three or more classes. The aim is to map data points to high dimensional space to gain mutual linear separation between every two classes. An application of this algorithm can be found in the following [paper of Manerkar et al. (2016)](https://www.proquest.com/docview/2456786692/6A78351CD3F64C9FPQ/4) where they use it in the context of automated skin disease segmentation and classification. ","metadata":{"id":"TUarBq2mrENc","papermill":{"duration":0.101099,"end_time":"2021-03-08T07:59:09.31402","exception":false,"start_time":"2021-03-08T07:59:09.212921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class MultiClassSVM:\n    \"\"\"An SVM classifier for face recognition\"\"\"\n\n    def __init__(self):\n        self.clf = None\n        self.evaluator = Evaluator()\n        \n    def cross_validate(self, features, train_y, n_splits=5):\n        scores = {'Accuracy': [], 'Recall': [], 'Precision': [], 'F1': []}\n        ss = ShuffleSplit(n_splits=n_splits, test_size=0.25)\n        for train_idx, test_idx in ss.split(f):\n            self.fit(features[train_idx], train_y[train_idx])  \n            train_y_star = self.predict(features[test_idx])\n            metric_dict = self.evaluator(train_y[test_idx], train_y_star)\n            for key, value in metric_dict.items():\n                scores[key].append(value)\n        for key, value in scores.items():\n            print('Average {}: {:.2f}%'.format(key, np.mean(value) * 100))\n            \n    def grid_search(self, features, train_y):\n        # initialize grid search params\n        C_range = np.arange(-2, 5, 0.5)               # logarithmic grid for c parameter\n        gamma_range = np.logspace(-9, 3, 10)            # log grid for gamma param\n        param_grid = dict(gamma=gamma_range, C=C_range)  # create a dict with these params\n        cv = ShuffleSplit(n_splits=8, test_size=0.2, random_state=0)\n        grid = GridSearchCV(                            # create hyperparameter search grid\n            SVC(kernel='linear', decision_function_shape='ovr', break_ties=False),\n            param_grid=param_grid, cv=cv)\n        grid.fit(features, train_y)                     # fit grid to training datas\n        print(f\"The best parameters are {grid.best_params_} with a score of {grid.best_score_}\")\n        \n\n    def fit(self, features, train_y):        \n        self.clf = make_pipeline(\n            StandardScaler(),\n            SVC(kernel='linear',\n                  C=10.0,\n                  gamma=0.01,\n                decision_function_shape='ovr',\n                break_ties=False))\n        self.clf.fit(features, train_y)\n\n    def predict(self, X):\n        return self.clf.predict(X)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"id":"_2PghXOtrENc","execution":{"iopub.status.busy":"2022-04-12T20:52:52.491851Z","iopub.execute_input":"2022-04-12T20:52:52.492426Z","iopub.status.idle":"2022-04-12T20:52:52.505349Z","shell.execute_reply.started":"2022-04-12T20:52:52.492392Z","shell.execute_reply":"2022-04-12T20:52:52.504386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Baseline 2: Random Forest 🌲🌳🌲\nRandom forest is a supervised machine learning algorithm widely used on classification problems. A random forest model consists of decision trees that were trained on different datasets which were row sampled from the training data. After predicting each instance on every decision tree, random forest takes the majority vote as the final output. This ensemble technique is called bagging (**B**ootstrap **Agg**regat**ing**). \n\nRandom forest is immune to overfitting and curse of dimensionality as each tree in the forest does not consider all the features.\n\n**Hyperparameters**\n\n* `n_estimators`: # of decision trees in the model\n* `max_depth`: The maximum depth of a decision tree\n* `njobs`: # of preprocessors to run, `-1` means all\n* `min_samples_split`: The minimum number of samples required to split an internal node further\n\nWe are going to use `GridSearchCV` to find the best hyperparameters for our dataset","metadata":{"id":"mm3sFn6UrENd","papermill":{"duration":0.101099,"end_time":"2021-03-08T07:59:09.31402","exception":false,"start_time":"2021-03-08T07:59:09.212921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class RandomForest:\n    def __init__(self):\n        self.clf = None\n        self.evaluator = Evaluator()\n\n    def cross_validate(self, features, train_y, n_splits=5):\n        scores = {'Accuracy': [], 'Recall': [], 'Precision': [], 'F1': []}\n        ss = ShuffleSplit(n_splits=n_splits, test_size=0.25)\n        for train_idx, test_idx in ss.split(f):\n            self.fit(features[train_idx], train_y[train_idx])  \n            train_y_star = self.predict(features[test_idx])\n            metric_dict = self.evaluator(train_y[test_idx], train_y_star)\n            for key, value in metric_dict.items():\n                scores[key].append(value)\n        for key, value in scores.items():\n            print('Average {}: {:.2f}%'.format(key, np.mean(value) * 100))\n            \n    def grid_search(self, features, train_y):\n        clf = RandomForestClassifier(random_state = 2)\n        param_grid = { \n            'max_depth': [3, 4, 5],\n            # 'min_samples_leaf': [1, 2, 4],\n            'min_samples_split': [2, 3, 5],\n            'n_estimators': [50, 100, 150, 200]\n        }\n        grid = GridSearchCV(clf, param_grid=param_grid, n_jobs=-1, cv=LeaveOneOut())\n        grid.fit(features, train_y)                     # fit grid to training datas\n        print(f\"The best parameters are {grid.best_params_} with a score of {grid.best_score_}\")\n\n    def fit(self, features, train_y):\n        self.clf = make_pipeline(\n            StandardScaler(),\n            RandomForestClassifier(max_depth=4, min_samples_split=2, n_estimators=100)\n        )\n        self.clf.fit(features, train_y)\n\n    def predict(self, X):\n        return self.clf.predict(X)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"id":"6lbPcHjrwitM","execution":{"iopub.status.busy":"2022-04-12T20:52:52.506954Z","iopub.execute_input":"2022-04-12T20:52:52.507306Z","iopub.status.idle":"2022-04-12T20:52:52.521338Z","shell.execute_reply.started":"2022-04-12T20:52:52.507264Z","shell.execute_reply":"2022-04-12T20:52:52.520417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4. Baseline 4: Binary SVM\n\nSupport Vector Machine classfieirs which functions in the same way that the MultiClass SVM classifiers works, however here the input is split. Two datasets are initialized with a SVM for each class. By having a classifier that focuses on one class at a time, there might be an improvement on the accuracy.\n\n\n**Hyperparameters**\n\n*   `C`: Regularization parameter. The strength of the regularization is inversely proportional to C.\n*   `gamma`: Kernel coefficient\n\nWe are going to use `GridSearchCV` to find the best hyperparameters for our dataset depending on leave one out cross-validation","metadata":{"id":"WoA-iZhirENd","papermill":{"duration":0.101099,"end_time":"2021-03-08T07:59:09.31402","exception":false,"start_time":"2021-03-08T07:59:09.212921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BSVM:\n    \"\"\"Create 2 simple linear Binary SVM classifiers, one for class 1 and one for class 2\"\"\"\n    def __init__(self):\n        self.clf1 = None\n        self.clf2 = None\n        self.evaluator = Evaluator()\n    \n    def cross_validate(self, features, train_y, n_splits=5):\n        scores = {'Accuracy': [], 'Recall': [], 'Precision': [], 'F1': []}\n        ss = ShuffleSplit(n_splits=n_splits, test_size=0.25)\n        for train_idx, test_idx in ss.split(f):\n            self.fit(features[train_idx], train_y[train_idx])  \n            train_y_star = self.predict(features[test_idx])\n            metric_dict = self.evaluator(train_y[test_idx], train_y_star)\n            for key, value in metric_dict.items():\n                scores[key].append(value)\n        for key, value in scores.items():\n            print('Average {}: {:.2f}%'.format(key, np.mean(value) * 100))\n    \n    def grid_search(self, features, train_y):\n        features_tmp = np.zeros_like(train_y)\n        features_tmp[train_y==1] = 1\n        C_range = np.logspace(-2, 10, 13)\n        gamma_range = np.logspace(-9, 3, 13)\n        param_grid = dict(gamma=gamma_range, C=C_range)\n        cv = LeaveOneOut()\n        cv.get_n_splits(features)\n        grid = GridSearchCV(SVC(kernel='linear', decision_function_shape='ovr', break_ties=False), param_grid=param_grid, n_jobs=-1, cv=cv)\n        grid.fit(features, features_tmp)\n        print(f\"The best parameters are {grid.best_params_} with a score of {grid.best_score_}\")\n        \n        features_tmp = np.zeros_like(train_y)\n        features_tmp[train_y==2] = 1\n        grid.fit(features, features_tmp)\n        print(f\"The best parameters are {grid.best_params_} with a score of {grid.best_score_}\")\n        \n    def fit(self, features, train_y):\n        # SVM class 1\n        features_tmp = np.zeros_like(train_y) \n        features_tmp[train_y==1] = 1        \n        self.clf1 = make_pipeline(\n            StandardScaler(),\n            SVC(kernel='linear', C=1, gamma=10**(-9), decision_function_shape='ovr', break_ties=False, probability=True))\n        self.clf1.fit(features, np.array(features_tmp))\n        \n        # SVM class 2\n        features_tmp = np.zeros_like(train_y)\n        features_tmp[train_y==2] = 1\n        self.clf2 = make_pipeline(\n            StandardScaler(),\n            SVC(kernel='linear', C=1, gamma=10**(-9), decision_function_shape='ovr', break_ties=False, probability=True))\n        self.clf2.fit(features, np.array(features_tmp)) \n        \n    def predict(self, features):\n        res1 = self.clf1.predict(features)\n        res2 = self.clf2.predict(features)\n        tmp = res1+2*res2 \n        prob1 = self.clf1.predict_proba(features)\n        prob2 = self.clf2.predict_proba(features)\n        for i,cls in enumerate(tmp):  \n            if cls==3:\n                if (prob1[i][1]>prob2[i][1]):\n                    tmp[i]=1\n                elif(prob1[i][1]<=prob2[i][1]):\n                    tmp[i]=2\n        return tmp\n        \n    def __call__(self, X):\n        return self.predict(X)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:52:52.522885Z","iopub.execute_input":"2022-04-12T20:52:52.523131Z","iopub.status.idle":"2022-04-12T20:52:52.540392Z","shell.execute_reply.started":"2022-04-12T20:52:52.5231Z","shell.execute_reply":"2022-04-12T20:52:52.539584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5. Baseline 5: MLP\nA multilayer perceptron (MLP) is a relatively small, fully connected, feedforward artificial neural network which consists of an input layer, at least one hidden layer and an output layer. MLPs are frequently used for classification of data (input) to a class (output) especially when the data is not linearly separable as an MLP is a universal approximator for polynomial functions. \n\nThis particular MLP has one hidden layer with 1024 hidden neurons. The network is trained by optimizing the log-loss function with [LBFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) in a supervised way. To avoid overfitting the features of the training data early stopping is used.","metadata":{"id":"X_I3cHr-rENd","papermill":{"duration":0.101099,"end_time":"2021-03-08T07:59:09.31402","exception":false,"start_time":"2021-03-08T07:59:09.212921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class MLP:\n    def __init__(self):\n        self.clf = None\n        self.evaluator = Evaluator()\n\n    def fit(self, features, train_y):\n        self.clf = make_pipeline(\n            StandardScaler(),\n            MLPClassifier(solver='lbfgs', hidden_layer_sizes = (1024,), batch_size = 50,\n                          verbose = False, early_stopping = True)\n        )\n        self.clf.fit(features, train_y)\n        \n    def cross_validate(self, features, train_y, n_splits=5):\n        scores = {'Accuracy': [], 'Recall': [], 'Precision': [], 'F1': []}\n        ss = ShuffleSplit(n_splits=n_splits, test_size=0.25)\n        for train_idx, test_idx in ss.split(f):\n            self.fit(features[train_idx], train_y[train_idx])  \n            train_y_star = self.predict(features[test_idx])\n            metric_dict = self.evaluator(train_y[test_idx], train_y_star)\n            for key, value in metric_dict.items():\n                scores[key].append(value)\n        for key, value in scores.items():\n            print('Average {}: {:.2f}%'.format(key, np.mean(value) * 100))\n\n    def predict(self, X):\n        return self.clf.predict(X)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"id":"-PH5Vtravu_u","execution":{"iopub.status.busy":"2022-04-12T20:52:52.541601Z","iopub.execute_input":"2022-04-12T20:52:52.542282Z","iopub.status.idle":"2022-04-12T20:52:52.55614Z","shell.execute_reply.started":"2022-04-12T20:52:52.54225Z","shell.execute_reply":"2022-04-12T20:52:52.555196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.6. Baseline 6: XGBoost\n\n**XGBoost** or e**X**treme **G**radient **Boost**ing is a framework that has seen a lot of popularity recently on *Kaggle* competitions. It has been used to great success for many teams to receive a high score on the leaderboard. <br>\nXGBoost is an open-source library which implements **gradient boosted decision trees**. It focuses on speed and performance.<br>\nOne of the main advantages of this classifier is its ease of parallelization which allows computers with multiple cores to run the classifier faster than would regularly be expected.<br>\n\n**Hyperparameters**\n\n*   `gamma`: Minimum loss reduction required to make a further partition on a leaf node of the tree.\n*   `learning_rate`: Step size shrinkage used in update to prevents overfitting.\n*   `subsample`: Subsample ratio of the training instances.\n*   `max_depth`: Maximum depth of a tree.\n*   `colsample_bytree`: Subsample ratio of columns when constructing each tree.","metadata":{"id":"I4fjPVbfkXxv"}},{"cell_type":"code","source":"class XGBoost:\n    \"\"\"Classification using XGBoost\"\"\"\n\n    def __init__(self):\n        self.clf = None\n        self.evaluator = Evaluator()\n        \n    def cross_validate(self, features, train_y, n_splits=5):\n        scores = {'Accuracy': [], 'Recall': [], 'Precision': [], 'F1': []}\n        ss = ShuffleSplit(n_splits=n_splits, test_size=0.25)\n        for train_idx, test_idx in ss.split(f):\n            self.fit(features[train_idx], train_y[train_idx])  \n            train_y_star = self.predict(features[test_idx])\n            metric_dict = self.evaluator(train_y[test_idx], train_y_star)\n            for key, value in metric_dict.items():\n                scores[key].append(value)\n        for key, value in scores.items():\n            print('Average {}: {:.2f}%'.format(key, np.mean(value) * 100))\n            \n    def grid_search(self, features, train_y):\n        # initialize grid search params\n        gamma_range = np.arange(0.01, 0.21, 0.05)\n        lr = np.arange(0.05,0.3,0.05)\n        ss = np.arange(0.3,0.7,0.05)\n        depth = np.arange(3,7,1)\n        cs = np.arange(0.5,1,0.25)\n        param_grid = dict(gamma=gamma_range, learning_rate=lr, subsample=ss, max_depth=depth, colsample_bytree=cs)\n        cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n        grid = GridSearchCV(XGBClassifier(base_score=0.33, booster='gbtree', colsample_bylevel=1,\n                      colsample_bynode=1, eval_metric='mlogloss',\n                      gpu_id=-1, importance_type='gain',\n                      interaction_constraints='',\n                      max_delta_step=0, min_child_weight=1, missing=np.nan,\n                      monotone_constraints='()', n_estimators=100,\n                      num_parallel_tree=1, objective='multi:softmax', random_state=0,\n                      reg_alpha=0, reg_lambda=1,\n                      tree_method='exact', use_label_encoder=False,\n                      validate_parameters=1, verbosity=None),\n                            param_grid=param_grid, n_jobs=-1, cv=cv, verbose=10)\n        grid.fit(features, train_y)\n        print(f\"The best parameters are {grid.best_params_} with a score of {grid.best_score_}\")\n        \n\n    def fit(self, features, train_y):\n        \n        self.clf = make_pipeline(\n            StandardScaler(),\n            XGBClassifier(base_score=0.33, booster='gbtree', colsample_bylevel=1,\n                      colsample_bynode=1, eval_metric='mlogloss',\n                      gpu_id=-1, importance_type='gain',\n                      interaction_constraints='',\n                      max_delta_step=0, min_child_weight=1, missing=np.nan,\n                      monotone_constraints='()', n_estimators=100,\n                      num_parallel_tree=1, objective='multi:softmax', random_state=0,\n                      reg_alpha=0, reg_lambda=1, \n                      gamma=0.2, learning_rate=0.15, subsample=0.5,\n                      max_depth=5, colsample_bytree=1,\n                      # gamma=grid.best_params_['gamma'], learning_rate=grid.best_params_['learning_rate'],\n                      # subsample=grid.best_params_['subsample'], max_depth=grid.best_params_['max_depth'],\n                      # colsample_bytree=grid.best_params_['colsample_bytree'],\n                      tree_method='exact', use_label_encoder=False,\n                      validate_parameters=1, verbosity=None))\n        self.clf.fit(features, train_y)                 # fit the classifier to training features\n\n    def predict(self, X):\n        return self.clf.predict(X)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"id":"c63AgY4Ckb47","execution":{"iopub.status.busy":"2022-04-12T20:52:52.557655Z","iopub.execute_input":"2022-04-12T20:52:52.55788Z","iopub.status.idle":"2022-04-12T20:52:52.575298Z","shell.execute_reply.started":"2022-04-12T20:52:52.557853Z","shell.execute_reply":"2022-04-12T20:52:52.574619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Experiments\n<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> Do <i>NOT</i> use this section to keep track of every little change you make in your code! Instead, highlight the most important findings and the major (best) pipelines that you've discovered.  \n</div>\n<br>\n\n## 4.1. Pipeline 1: MultiClass SVM with Facial Encodings\n","metadata":{"id":"E44Lxv6MrENd","papermill":{"duration":0.101099,"end_time":"2021-03-08T07:59:09.31402","exception":false,"start_time":"2021-03-08T07:59:09.212921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n\npreprocessor = FaceRec_Preprocessor(face_size=FACE_SIZE, faces=-1)\nfeature_extractor = FEncodings_FeatureExtractor()\nclassifier = MultiClassSVM()\ncross_val = 10\n\nprint('Preprocessing training data')\ntrain_X, train_y, ids = preprocessor(train, train['class'].values, label='train')\nprint('Calculating feature for training dataset')\ntrain_features = feature_extractor(train_X, label='train', preproc='FACEREC')\nprint('Cleaning the training dataset')\ncleaner = CleanTrainingSet(train_X, train_y, ids, train_features)\ntrain_X = cleaner.clean(label='train', feature='FE', preproc='FACEREC')\ntrain_y = cleaner.get_class()\ntrain_X, train_y = remove_blacks(train_X, train_y)\n\nprint('Augmenting training dataset')\ntrain_X, train_y = augmenter(train_X,train_y)\n\nprint('Preprocessing test data')\ntest_X, _, test_ids = preprocessor(test, label='test')\nprint('Calculating feature for test dataset')\ntest_features = feature_extractor(test_X, label='test', preproc='FACEREC')\nprint('Cleaning the test dataset')\ncleaner = CleanTrainingSet(test_X, None, test_ids, test_features)\ntest_X = cleaner.clean(train_X=train_X, train_y=train_y, train_features=feature_extractor(train_X, label='train',\n                                                                                          preproc='FACEREC_clean_augm'),\n                      label='test', feature='FE', preproc='FACEREC_augm')\n# train the model on the features\nf = feature_extractor(train_X, label='train', preproc='FACEREC_clean_augm')\nprint('Fitting to input features\\n')\nprint('Performance of model on the training set with {}-fold cross validation'.format(cross_val))\nclassifier.cross_validate(f, train_y, n_splits=cross_val)","metadata":{"id":"Klp51qYYrENe","papermill":{"duration":0.430319,"end_time":"2021-03-08T07:59:10.263691","exception":false,"start_time":"2021-03-08T07:59:09.833372","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:52:52.576594Z","iopub.execute_input":"2022-04-12T20:52:52.577016Z","iopub.status.idle":"2022-04-12T20:52:53.670266Z","shell.execute_reply.started":"2022-04-12T20:52:52.576983Z","shell.execute_reply":"2022-04-12T20:52:53.669512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. Pipeline 2: RandomForest with Facial Encodings","metadata":{}},{"cell_type":"code","source":"%%time\n\npreprocessor = FaceRec_Preprocessor(face_size=FACE_SIZE, faces=-1)\nfeature_extractor = FEncodings_FeatureExtractor()\nclassifier = RandomForest()\ncross_val = 10\n\nprint('Preprocessing training data')\ntrain_X, train_y, ids = preprocessor(train, train['class'].values, label='train')\nprint('Calculating feature for training dataset')\ntrain_features = feature_extractor(train_X, label='train', preproc='FACEREC')\nprint('Cleaning the training dataset')\ncleaner = CleanTrainingSet(train_X, train_y, ids, train_features)\ntrain_X = cleaner.clean(label='train', feature='FE', preproc='FACEREC')\ntrain_y = cleaner.get_class()\ntrain_X, train_y = remove_blacks(train_X, train_y)\n\nprint('Augmenting training dataset')\ntrain_X, train_y = augmenter(train_X,train_y)\n\nprint('Preprocessing test data')\ntest_X, _, test_ids = preprocessor(test, label='test')\nprint('Calculating feature for test dataset')\ntest_features = feature_extractor(test_X, label='test', preproc='FACEREC')\nprint('Cleaning the test dataset')\ncleaner = CleanTrainingSet(test_X, None, test_ids, test_features)\ntest_X = cleaner.clean(train_X=train_X, train_y=train_y, train_features=feature_extractor(train_X, label='train',\n                                                                                          preproc='FACEREC_clean_augm'),\n                      label='test', feature='FE', preproc='FACEREC_augm')\n# train the model on the features\nf = feature_extractor(train_X, label='train', preproc='FACEREC_clean_augm')\nprint('Fitting to input features\\n')\nprint('Performance of model on the training set with {}-fold cross validation'.format(cross_val))\nclassifier.cross_validate(f, train_y)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T20:52:53.671364Z","iopub.execute_input":"2022-04-12T20:52:53.671592Z","iopub.status.idle":"2022-04-12T20:52:54.761725Z","shell.execute_reply.started":"2022-04-12T20:52:53.671561Z","shell.execute_reply":"2022-04-12T20:52:54.760854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3. Pipeline 3: XGBoost with Facial Encodings","metadata":{}},{"cell_type":"code","source":"%%time\n\npreprocessor = FaceRec_Preprocessor(face_size=FACE_SIZE, faces=-1)\nfeature_extractor = FEncodings_FeatureExtractor()\nclassifier = XGBoost()\ncross_val = 10\n\nprint('Preprocessing training data')\ntrain_X, train_y, ids = preprocessor(train, train['class'].values, label='train')\nprint('Calculating feature for training dataset')\ntrain_features = feature_extractor(train_X, label='train', preproc='FACEREC')\nprint('Cleaning the training dataset')\ncleaner = CleanTrainingSet(train_X, train_y, ids, train_features)\ntrain_X = cleaner.clean(label='train', feature='FE', preproc='FACEREC')\ntrain_y = cleaner.get_class()\ntrain_X, train_y = remove_blacks(train_X, train_y)\n\nprint('Augmenting training dataset')\ntrain_X, train_y = augmenter(train_X,train_y)\n\nprint('Preprocessing test data')\ntest_X, _, test_ids = preprocessor(test, label='test')\nprint('Calculating feature for test dataset')\ntest_features = feature_extractor(test_X, label='test', preproc='FACEREC')\nprint('Cleaning the test dataset')\ncleaner = CleanTrainingSet(test_X, None, test_ids, test_features)\ntest_X = cleaner.clean(train_X=train_X, train_y=train_y, train_features=feature_extractor(train_X, label='train',\n                                                                                          preproc='FACEREC_clean_augm'),\n                      label='test', feature='FE', preproc='FACEREC_augm')\n# train the model on the features\nf = feature_extractor(train_X, label='train', preproc='FACEREC_clean_augm')\nprint('Fitting to input features\\n')\nprint('Performance of model on the training set with {}-fold cross validation'.format(cross_val))\nclassifier.cross_validate(f, train_y, n_splits=cross_val)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:52:54.763467Z","iopub.execute_input":"2022-04-12T20:52:54.763754Z","iopub.status.idle":"2022-04-12T20:53:03.605436Z","shell.execute_reply.started":"2022-04-12T20:52:54.763723Z","shell.execute_reply":"2022-04-12T20:53:03.604245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4. Pipeline 4: MLP with Facial Encodings","metadata":{}},{"cell_type":"code","source":"%%time\n\npreprocessor = FaceRec_Preprocessor(face_size=FACE_SIZE, faces=-1)\nfeature_extractor = FEncodings_FeatureExtractor()\nclassifier = MLP()\ncross_val = 10\n\nprint('Preprocessing training data')\ntrain_X, train_y, ids = preprocessor(train, train['class'].values, label='train')\nprint('Calculating feature for training dataset')\ntrain_features = feature_extractor(train_X, label='train', preproc='FACEREC')\nprint('Cleaning the training dataset')\ncleaner = CleanTrainingSet(train_X, train_y, ids, train_features)\ntrain_X = cleaner.clean(label='train', feature='FE', preproc='FACEREC')\ntrain_y = cleaner.get_class()\ntrain_X, train_y = remove_blacks(train_X, train_y)\n\nprint('Augmenting training dataset')\ntrain_X, train_y = augmenter(train_X,train_y)\n\nprint('Preprocessing test data')\ntest_X, _, test_ids = preprocessor(test, label='test')\nprint('Calculating feature for test dataset')\ntest_features = feature_extractor(test_X, label='test', preproc='FACEREC')\nprint('Cleaning the test dataset')\ncleaner = CleanTrainingSet(test_X, None, test_ids, test_features)\ntest_X = cleaner.clean(train_X=train_X, train_y=train_y, train_features=feature_extractor(train_X, label='train',\n                                                                                          preproc='FACEREC_clean_augm'),\n                      label='test', feature='FE', preproc='FACEREC_augm')\n# train the model on the features\nf = feature_extractor(train_X, label='train', preproc='FACEREC_clean_augm')\nprint('Fitting to input features\\n')\nprint('Performance of model on the training set with {}-fold cross validation'.format(cross_val))\nclassifier.cross_validate(f, train_y, n_splits=cross_val)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:53:03.606878Z","iopub.execute_input":"2022-04-12T20:53:03.607219Z","iopub.status.idle":"2022-04-12T20:53:29.099351Z","shell.execute_reply.started":"2022-04-12T20:53:03.607165Z","shell.execute_reply":"2022-04-12T20:53:29.098282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4. Pipeline 4: Binary SVM with Facial Encodings","metadata":{}},{"cell_type":"code","source":"%%time\n\npreprocessor = FaceRec_Preprocessor(face_size=FACE_SIZE, faces=-1)\nfeature_extractor = FEncodings_FeatureExtractor()\nclassifier = BSVM()\ncross_val = 10\n\nprint('Preprocessing training data')\ntrain_X, train_y, ids = preprocessor(train, train['class'].values, label='train')\nprint('Calculating feature for training dataset')\ntrain_features = feature_extractor(train_X, label='train', preproc='FACEREC')\nprint('Cleaning the training dataset')\ncleaner = CleanTrainingSet(train_X, train_y, ids, train_features)\ntrain_X = cleaner.clean(label='train', feature='FE', preproc='FACEREC')\ntrain_y = cleaner.get_class()\ntrain_X, train_y = remove_blacks(train_X, train_y)\n\nprint('Augmenting training dataset')\ntrain_X, train_y = augmenter(train_X,train_y)\n\nprint('Preprocessing test data')\ntest_X, _, test_ids = preprocessor(test, label='test')\nprint('Calculating feature for test dataset')\ntest_features = feature_extractor(test_X, label='test', preproc='FACEREC')\nprint('Cleaning the test dataset')\ncleaner = CleanTrainingSet(test_X, None, test_ids, test_features)\ntest_X = cleaner.clean(train_X=train_X, train_y=train_y, train_features=feature_extractor(train_X, label='train',\n                                                                                          preproc='FACEREC_clean_augm'),\n                      label='test', feature='FE', preproc='FACEREC_augm')\n# train the model on the features\nf = feature_extractor(train_X, label='train', preproc='FACEREC_clean_augm')\nprint('Fitting to input features\\n')\nprint('Performance of model on the training set with {}-fold cross validation'.format(cross_val))\nclassifier.cross_validate(f, train_y)","metadata":{"id":"Klp51qYYrENe","papermill":{"duration":0.430319,"end_time":"2021-03-08T07:59:10.263691","exception":false,"start_time":"2021-03-08T07:59:09.833372","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:53:29.105796Z","iopub.execute_input":"2022-04-12T20:53:29.106514Z","iopub.status.idle":"2022-04-12T20:53:29.986531Z","shell.execute_reply.started":"2022-04-12T20:53:29.106449Z","shell.execute_reply":"2022-04-12T20:53:29.98564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4. Pipeline 4: MultiClassSVM with HOG","metadata":{}},{"cell_type":"code","source":"%%time\n\npreprocessor = FaceRec_Preprocessor(face_size=FACE_SIZE, faces=-1)\nfeature_extractor = HOGFeatureExtractor()\nclassifier = MultiClassSVM()\ncross_val = 10\n\nprint('Preprocessing training data')\ntrain_X, train_y, ids = preprocessor(train, train['class'].values, label='train')\nprint('Calculating feature for training dataset')\ntrain_features = feature_extractor(train_X, label='train', preproc='FACEREC')\nprint('Cleaning the training dataset')\ncleaner = CleanTrainingSet(train_X, train_y, ids, train_features)\ntrain_X = cleaner.clean(label='train', feature='HOG', preproc='FACEREC')\ntrain_y = cleaner.get_class()\ntrain_X, train_y = remove_blacks(train_X, train_y)\n\nprint('Augmenting training dataset')\ntrain_X, train_y = augmenter(train_X,train_y)\n\nprint('Preprocessing test data')\ntest_X, _, test_ids = preprocessor(test, label='test')\nprint('Calculating feature for test dataset')\ntest_features = feature_extractor(test_X, label='test', preproc='FACEREC')\nprint('Cleaning the test dataset')\ncleaner = CleanTrainingSet(test_X, None, test_ids, test_features)\ntest_X = cleaner.clean(train_X=train_X, train_y=train_y, train_features=feature_extractor(train_X, label='train',\n                                                                                          preproc='FACEREC_clean_augm'),\n                      label='test', feature='HOG', preproc='FACEREC_augm')\n# train the model on the features\nf = feature_extractor(train_X, label='train', preproc='FACEREC_clean_augm')\nprint('Fitting to input features\\n')\nprint('Performance of model on the training set with {}-fold cross validation'.format(cross_val))\nclassifier.cross_validate(f, train_y, n_splits=cross_val)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:53:29.988243Z","iopub.execute_input":"2022-04-12T20:53:29.988575Z","iopub.status.idle":"2022-04-12T20:55:03.919568Z","shell.execute_reply.started":"2022-04-12T20:53:29.98853Z","shell.execute_reply":"2022-04-12T20:55:03.918669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Publishing best results","metadata":{"id":"0n1agOiqrENe","papermill":{"duration":0.103853,"end_time":"2021-03-08T07:59:10.903341","exception":false,"start_time":"2021-03-08T07:59:10.799488","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n\npreprocessor = FaceRec_Preprocessor(face_size=FACE_SIZE, faces=-1)\nfeature_extractor = FEncodings_FeatureExtractor()\nclassifier = BSVM()\ncross_val = 10\n\nprint('Preprocessing training data')\ntrain_X, train_y, ids = preprocessor(train, train['class'].values, label='train')\nprint('Calculating feature for training dataset')\ntrain_features = feature_extractor(train_X, label='train', preproc='FACEREC')\nprint('Cleaning the training dataset')\ncleaner = CleanTrainingSet(train_X, train_y, ids, train_features)\ntrain_X = cleaner.clean(label='train', feature='FE', preproc='FACEREC')\ntrain_y = cleaner.get_class()\ntrain_X, train_y = remove_blacks(train_X, train_y)\n\nprint('Augmenting training dataset')\ntrain_X, train_y = augmenter(train_X,train_y)\n\nprint('Preprocessing test data')\ntest_X, _, test_ids = preprocessor(test, label='test')\nprint('Calculating feature for test dataset')\ntest_features = feature_extractor(test_X, label='test', preproc='FACEREC')\nprint('Cleaning the test dataset')\ncleaner = CleanTrainingSet(test_X, None, test_ids, test_features)\ntest_X = cleaner.clean(train_X=train_X, train_y=train_y, train_features=feature_extractor(train_X, label='train',\n                                                                                          preproc='FACEREC_clean_augm'),\n                      label='test', feature='FE', preproc='FACEREC_augm')\n# train the model on the features\nf = feature_extractor(train_X, label='train', preproc='FACEREC_clean_augm')\nprint('Fitting to input features\\n')\nprint('Performance of model on the training set with {}-fold cross validation'.format(cross_val))\nclassifier.fit(f, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:55:04.067815Z","iopub.execute_input":"2022-04-12T20:55:04.068309Z","iopub.status.idle":"2022-04-12T20:55:04.802267Z","shell.execute_reply.started":"2022-04-12T20:55:04.068262Z","shell.execute_reply":"2022-04-12T20:55:04.801322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the labels for the test set \ntest_y_star = classifier(feature_extractor(test_X, label='test', preproc='FACEREC_clean_augm'))","metadata":{"id":"oucsKB76rENe","papermill":{"duration":0.111828,"end_time":"2021-03-08T07:59:10.696438","exception":false,"start_time":"2021-03-08T07:59:10.58461","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:55:04.803454Z","iopub.execute_input":"2022-04-12T20:55:04.803763Z","iopub.status.idle":"2022-04-12T20:55:04.839875Z","shell.execute_reply.started":"2022-04-12T20:55:04.803731Z","shell.execute_reply":"2022-04-12T20:55:04.839165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test.copy().drop('img', axis = 1)\nsubmission['class'] = test_y_star\n\nsubmission","metadata":{"id":"cEpAspyGrENe","papermill":{"duration":0.120392,"end_time":"2021-03-08T07:59:11.127762","exception":false,"start_time":"2021-03-08T07:59:11.00737","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:55:04.841521Z","iopub.execute_input":"2022-04-12T20:55:04.841909Z","iopub.status.idle":"2022-04-12T20:55:04.857649Z","shell.execute_reply.started":"2022-04-12T20:55:04.841864Z","shell.execute_reply":"2022-04-12T20:55:04.856433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"id":"6s_l8cALrENf","papermill":{"duration":0.122516,"end_time":"2021-03-08T07:59:11.356409","exception":false,"start_time":"2021-03-08T07:59:11.233893","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T20:55:04.859062Z","iopub.execute_input":"2022-04-12T20:55:04.859312Z","iopub.status.idle":"2022-04-12T20:55:04.870009Z","shell.execute_reply.started":"2022-04-12T20:55:04.859281Z","shell.execute_reply":"2022-04-12T20:55:04.869184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Discussion\nIn this group assignment we had the opportunity to experiment with the face recognition task, using various data preprocessing and feature extraction techniques. HAAR was already given as a face detection algorithm, however we wanted try different techniques such as InsightFace and HAAR with multi face detention to improve the quality of the training data. Among all preprocessors we tried, FaceRecognition was the most successful one that provides a good quality data set after cleaning the data and removing black images.\n\nAs feature extraction, we looked into five different methods: HOG, PCA, FAST, SIFT, FaceNet and Facial Encodings. The t-SNE plots of HOG, FAST and SIFT were not promising, i.e. clusters were not clearly separable in 2-D. Therefore, we decided to use facial encodings as feature.\n\nAs evalution metrics, we noticed that accuracy by itself is not the most successful metric. Therefore, we used precision and recall (so F1 Score) as evaluation metric in addition to accuracy to choose the right classifier.\n\nFive classifiers were implemented, namely, Multi-class SVM, Double binary SVM, Multi-layer Perceptrons (MLP),Random Forest and XGBoost. By combining all preprocessors, all feature detectors and all classifiers, the highest score we got is `94.603%` with Random Forest having `(max_depth=4, min_samples_split=2, n_estimators=100)` hyperparameters after experimenting with the grid search. \n\nThis is an optimal score considering the capabilities of the model and size of the training data we have. Although if we had more time, we would try deep learning methods such as Convolutional Neural Networks. In order to train a neural network, we would need a larger training data which can be generated by data augmentation.\n\n","metadata":{"id":"Fr22gB3irENf","papermill":{"duration":0.116655,"end_time":"2021-03-08T07:59:11.577703","exception":false,"start_time":"2021-03-08T07:59:11.461048","status":"completed"},"tags":[]}}]}